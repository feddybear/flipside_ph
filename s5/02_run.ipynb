{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: expected 35604 data files, found 35606.\n",
      "FSC data preparation succeeded.\n",
      "utils/fix_data_dir.sh: filtered data/train/segments from 617902 to 616756 lines based on filter /tmp/kaldi.jxRb/recordings.\n",
      "utils/fix_data_dir.sh: filtered data/train/wav.scp from 35606 to 35604 lines based on filter /tmp/kaldi.jxRb/recordings.\n",
      "fix_data_dir.sh: kept 326122 utterances out of 617902\n",
      "utils/fix_data_dir.sh: filtered data/train/wav.scp from 35604 to 35601 lines based on filter /tmp/kaldi.jxRb/recordings.\n",
      "fix_data_dir.sh: old files are kept in data/train/.backup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cat: data/FSC/lexicon/lexicon.txt: No such file or directory\n",
      "cat: /storage07/user_data/angfederico01/local/g2p-seq2seq_OLD/KIT_dict.norm: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.21 ms, sys: 949 µs, total: 5.16 ms\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    "set -e\n",
    "\n",
    "local/fph_data_prep.sh data/FSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.91 ms, sys: 2.5 ms, total: 4.41 ms\n",
      "Wall time: 1.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    "set -e\n",
    "\n",
    "FSC=data/FSC\n",
    "ISIP=$(echo $FSC | sed 's/FSC/ISIP_TGL/g')\n",
    "KIT=$(echo $FSC | sed 's/FSC/KIT_TGL/g')\n",
    "\n",
    "\n",
    "\n",
    "# Audio data directory check\n",
    "if [ ! -d $FSC ]; then\n",
    " echo \"Error: run.sh requires a directory argument\"\n",
    "  exit 1;\n",
    "fi\n",
    "\n",
    "export LC_ALL=C;\n",
    "# FSC dictionary file check\n",
    "#if [ ! -f $dir/lexicon.txt ]; then\n",
    "#  cat data/local/dict/finaldict.txt | sort --parallel=8 | uniq >$dir/lexicon.txt || exit 1;\n",
    "#fi\n",
    "\n",
    "fscdir=data/local/fsc\n",
    "rm -fr $fscdir\n",
    "mkdir -p $fscdir\n",
    "cat $FSC/*/*/*-wav.list 2>/dev/null | sed '/^$/d' | sed '/01_xx01xxxx_14A/d' |\\\n",
    "    sed '/04_xx10xxxx_22B/d' | sed '/12_xx10xxxx_14B/d' |\\\n",
    "    sed 's/ /\\\\ /g' | sed 's/\\([()]\\)/\\\\\\1/g' > $fscdir/wav.flist # Using All data\n",
    "\n",
    "isipdir=data/local/isip\n",
    "rm -fr $isipdir\n",
    "mkdir -p $isipdir\n",
    "ls $ISIP/*/*/*-wav.list | grep -v '[Ss]po' | xargs cat 2>/dev/null | sed '/^$/d' | sed 's/ /\\\\ /g' |\\\n",
    "    sort --parallel=8 | uniq > $isipdir/wav.flist\n",
    "\n",
    "kitdir=data/local/kit\n",
    "rm -fr $kitdir\n",
    "mkdir -p $kitdir\n",
    "cat $KIT/*/*/*-wav.list 2>/dev/null | sed '/^$/d' | sort --parallel=8 | uniq > $kitdir/wav.flist\n",
    "\n",
    "\n",
    "\n",
    "# transcriptions\n",
    "cat $FSC/*/*/*-trn.txt |\\\n",
    "    sed '/01_xx01xxxx_14A/d' | sed '/04_xx10xxxx_22B/d' | sed '/12_xx10xxxx_14B/d' |\\\n",
    "    awk '{if (($4 !~ /^\\.\\.$/) && ($4 !~ /^$/) && ($4 !~ /^<sp>$/)) \\\n",
    "        {printf \"%s \",$1; for (i=4; i<=NF; i++) printf \" %s\",$i; printf \"\\n\"}}' |\\\n",
    "    sed 's/AS/C/g; s/PB/O/g; s/PBAS/O/g' | sort --parallel=8 > $fscdir/text\n",
    "\n",
    "ls $ISIP/*/*/*-trn.txt | grep -v '[Ss]po' | xargs cat 2>/dev/null | awk -F'\\t' '{printf \"%s  %s\\n\",$1,$4}' |\\\n",
    "    sort --parallel=8 > $isipdir/text\n",
    "\n",
    "awk -F'\\t' '{printf \"%s  %s\\n\",$1,$4}' $KIT/*/*/*-trn.txt | sort --parallel=8 > $kitdir/text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 4.43 ms, total: 4.43 ms\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    "set -e\n",
    "\n",
    "FSC=data/FSC\n",
    "ISIP=$(echo $FSC | sed 's/FSC/ISIP_TGL/g')\n",
    "KIT=$(echo $FSC | sed 's/FSC/KIT_TGL/g')\n",
    "\n",
    "fscdir=data/local/fsc\n",
    "export LC_ALL=C;\n",
    "# segments\n",
    "cat $FSC/*/*/*-trn.txt |\\\n",
    "    sed '/01_xx01xxxx_14A/d' | sed '/04_xx10xxxx_22B/d' |\\\n",
    "    sed '/12_xx10xxxx_14B/d' | sed 's/AS/C/g; s/PB/O/g; s/PBAS/O/g' |\\\n",
    "    awk '{if (($4 !~ /^\\.\\.$/) && ($4 !~ /^$/) && ($4 !~ /^<sp>$/)) { \\\n",
    "\tif ($3 > $2) {\n",
    "\t  segment=$1\n",
    "\t  split(segment,S,\"[_]\");\n",
    "\t  spkid=S[1]\"_\"S[2]\"_\"S[3];\n",
    "\t  print segment \" \" spkid \" \" $2 \" \" $3\n",
    "\t}\n",
    "}}' | sort --parallel=8 > $fscdir/segments\n",
    "\n",
    "isipdir=data/local/isip\n",
    "ls $ISIP/*/*/*-trn.txt | grep -v '[Ss]po' | xargs cat 2>/dev/null |\\\n",
    "awk '{\n",
    "          segment=$1\n",
    "          #split(segment,S,\"[.]\");\n",
    "          #spkid=S[1]\".\"S[2];\n",
    "          print segment \" \" segment \" \" $2 \" \" $3\n",
    "}' | sort --parallel=8 > $isipdir/segments\n",
    "\n",
    "kitdir=data/local/kit\n",
    "awk '{\n",
    "          segment=$1\n",
    "          split(segment,S,\"[.]\");\n",
    "          spkid=S[1]\".\"S[2];\n",
    "          print segment \" \" spkid \" \" $2 \" \" $3\n",
    "}' <(cat $KIT/*/*/*-trn.txt) | sort --parallel=8 > $kitdir/segments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 754 µs, sys: 3.15 ms, total: 3.91 ms\n",
      "Wall time: 617 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "export LC_ALL=C;\n",
    "fscdir=data/local/fsc\n",
    "awk '{segment=$1; split(segment,S,\"[_]\"); spkid=S[1]\"_\"S[2]\"_\"S[3]; print $1 \" \" spkid}' $fscdir/segments |\\\n",
    "    sort --parallel=8 -k2 > $fscdir/utt2spk || exit 1;\n",
    "\n",
    "isipdir=data/local/isip\n",
    "awk '{segment=$1; split(segment,S,\"[.]\"); spkid=S[1]\".\"S[2]\".\"S[3]\".\"S[4]; print $1 \" \" spkid}' $isipdir/segments |\\\n",
    "    sort --parallel=8 -k2 > $isipdir/utt2spk || exit 1;\n",
    "\n",
    "kitdir=data/local/kit\n",
    "awk '{segment=$1; split(segment,S,\"[.]\"); spkid=S[1]\".\"S[2]; print $1 \" \" spkid}' $kitdir/segments |\\\n",
    "    sort --parallel=8 -k2 > $kitdir/utt2spk || exit 1;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.35 ms, sys: 946 µs, total: 4.29 ms\n",
      "Wall time: 237 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "export LC_ALL=C;\n",
    "fscdir=data/local/fsc\n",
    "sed -e 's?.*/??' -e 's?.wav??' -e 's/PBAS/O/g; s/AS/C/g; s/PB/O/g;' $fscdir/wav.flist |\\\n",
    "    paste - $fscdir/wav.flist | sort --parallel=8 > $fscdir/wavflist.scp\n",
    "\n",
    "isipdir=data/local/isip\n",
    "sed -e 's?.*/??' -e 's?.wav??' $isipdir/wav.flist | paste - $isipdir/wav.flist |\\\n",
    "    sort --parallel=8 > $isipdir/wavflist.scp\n",
    "\n",
    "kitdir=data/local/kit\n",
    "sed -e 's?.*/\\([^/]*\\)/\\([^/]*$\\)?\\1.\\2?' -e 's?.wav??' $kitdir/wav.flist |\\\n",
    "    paste - $kitdir/wav.flist | sort --parallel=8 > $kitdir/wavflist.scp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    "set -e\n",
    "\n",
    "dir=data/local/train\n",
    "rm -fr $dir\n",
    "mkdir -p $dir\n",
    "\n",
    "export LC_ALL=C;\n",
    "# FSC dictionary file check\n",
    "if [ ! -f $dir/lexicon.txt ]; then\n",
    "  cat data/local/dict/finaldict.txt | sort --parallel=8 | uniq >$dir/lexicon.txt || exit 1;\n",
    "fi\n",
    "\n",
    "fscdir=data/local/fsc\n",
    "isipdir=data/local/isip\n",
    "kitdir=data/local/kit\n",
    "\n",
    "# recordings\n",
    "cat $fscdir/wav.flist $isipdir/wav.flist $kitdir/wav.flist | sort --parallel=8 > $dir/wav.flist\n",
    "\n",
    "# trans\n",
    "cat $fscdir/text $isipdir/text $kitdir/text | sort --parallel=8 > $dir/text\n",
    "\n",
    "# time start, time ends\n",
    "cat $fscdir/segments $isipdir/segments $kitdir/segments | sort --parallel=8 > $dir/segments\n",
    "\n",
    "cat $fscdir/utt2spk $isipdir/utt2spk $kitdir/utt2spk | sort --parallel=8 > $dir/utt2spk\n",
    "\n",
    "cat $fscdir/wavflist.scp $isipdir/wavflist.scp $kitdir/wavflist.scp | sort --parallel=8 > $dir/wavflist.scp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "dir=data/local/train\n",
    "\n",
    "awk '{\n",
    " printf(\"%s cat \", $1);\n",
    " for (i=2;i<=NF;i++) {\n",
    "    printf(\"%s \",$i)\n",
    " }\n",
    " printf(\"|\\n\")\n",
    "}' < $dir/wavflist.scp | sort > $dir/wav.scp || exit 1;\n",
    "\n",
    "sort -k 2 $dir/utt2spk | utils/utt2spk_to_spk2utt.pl > $dir/spk2utt || exit 1;\n",
    "\n",
    "# Copy stuff into its final locations [this has been moved from the format_data script]\n",
    "rm -fr data/train\n",
    "mkdir -p data/train\n",
    "for f in spk2utt utt2spk wav.scp text segments; do\n",
    "  cp data/local/train/$f data/train/ || exit 1;\n",
    "done\n",
    "\n",
    "echo \"FSC data preparation succeeded.\"\n",
    "\n",
    "utils/fix_data_dir.sh data/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    "set -e\n",
    "\n",
    "local/fph_prepare_dict.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    "set -e # exit on error\n",
    "\n",
    "rm -fr data/local/lang_nosilp\n",
    "rm -fr data/lang_nosilp\n",
    "utils/prepare_lang.sh --num-sil-states 4 data/local/dict_nosilp \"<unk>\" data/local/lang_nosilp data/lang_nosilp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    "set -e # exit on error\n",
    "\n",
    "# Now make MFCC features.\n",
    "# mfccdir should be some place with a largish disk where you\n",
    "# want to store MFCC features.\n",
    "mfccdir=mfcc\n",
    "\n",
    "for x in train; do\n",
    "  steps/make_mfcc.sh --nj 24 --cmd \"$train_cmd\" \\\n",
    "    data/$x exp/make_mfcc/$x $mfccdir\n",
    "  steps/compute_cmvn_stats.sh data/$x exp/make_mfcc/$x $mfccdir\n",
    "  utils/fix_data_dir.sh data/$x\n",
    "done\n",
    "\n",
    "echo \"Finish creating MFCCs\"\n",
    "\n",
    "#SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    "set -e # exit on error\n",
    "\n",
    "#: << '#SKIP'\n",
    "\n",
    "use_dev=false # Use the first 4k sentences from training data as dev set. (39 speakers.)\n",
    "\n",
    "##### Training and Decoding steps start from here #####\n",
    "\n",
    "# Use the first 4k sentences as dev set.  Note: when we trained the LM, we used\n",
    "# the 1st 10k sentences as dev set, so the 1st 4k won't have been used in the\n",
    "# LM training data.   However, they will be in the lexicon, plus speakers\n",
    "# may overlap, so it's still not quite equivalent to a test set.\n",
    "rm -fr data/train_nodev\n",
    "if $use_dev ;then\n",
    "    #dev_set=train_dev\n",
    "    #utils/subset_data_dir.sh --first data/train 4000 data/$dev_set # 6hr 31min\n",
    "    #n=$[`cat data/train/segments | wc -l` - 4000]\n",
    "    #utils/subset_data_dir.sh --last data/train $n data/train_nodev\n",
    "    cp -r data/train data/train_nodev\n",
    "else\n",
    "    cp -r data/train data/train_nodev\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    "set -e # exit on error\n",
    "\n",
    "## Starting basic training on MFCC features\n",
    "rm -fr exp/mono\n",
    "steps/train_mono.sh --nj 24 --cmd \"$train_cmd\" \\\n",
    "  data/train_nodev data/lang_nosilp exp/mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    "set -e # exit on error\n",
    "rm -fr exp/mono_ali\n",
    "rm -fr exp/tri1\n",
    "\n",
    "steps/align_si.sh --nj 24 --cmd \"$train_cmd\" \\\n",
    "  data/train_nodev data/lang_nosilp exp/mono exp/mono_ali\n",
    "\n",
    "steps/train_deltas.sh --cmd \"$train_cmd\" \\\n",
    "  3200 30000 data/train_nodev data/lang_nosilp exp/mono_ali exp/tri1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    "set -e # exit on error\n",
    "\n",
    "rm -fr exp/tri1_ali\n",
    "rm -fr exp/tri2\n",
    "\n",
    "steps/align_si.sh --nj 24 --cmd \"$train_cmd\" \\\n",
    "  data/train_nodev data/lang_nosilp exp/tri1 exp/tri1_ali\n",
    "\n",
    "steps/train_deltas.sh --cmd \"$train_cmd\" \\\n",
    "  4000 70000 data/train_nodev data/lang_nosilp exp/tri1_ali exp/tri2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    "set -e # exit on error\n",
    "\n",
    "# From now, we start using all of the data (except some duplicates of common\n",
    "# utterances, which don't really contribute much).\n",
    "steps/align_si.sh --nj 24 --cmd \"$train_cmd\" \\\n",
    "  data/train_nodev data/lang_nosilp exp/tri2 exp/tri2_ali\n",
    "\n",
    "# Do another iteration of LDA+MLLT training, on all the data.\n",
    "steps/train_lda_mllt.sh --cmd \"$train_cmd\" \\\n",
    "  6000 140000 data/train_nodev data/lang_nosilp exp/tri2_ali exp/tri3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    "set -e # exit on error\n",
    "\n",
    "# Now we compute the pronunciation and silence probabilities from training data,\n",
    "# and re-create the lang directory.\n",
    "steps/get_prons.sh --cmd \"$train_cmd\" data/train_nodev data/lang_nosilp exp/tri3\n",
    "utils/dict_dir_add_pronprobs.sh --max-normalize true \\\n",
    "  data/local/dict_nosilp exp/tri3/pron_counts_nowb.txt exp/tri3/sil_counts_nowb.txt \\\n",
    "  exp/tri3/pron_bigram_counts_nowb.txt data/local/dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    "set -e # exit on error\n",
    "\n",
    "# Now we re-create the lang directory.\n",
    "utils/prepare_lang.sh data/local/dict \"<unk>\" data/local/lang data/lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    "set -e # exit on error\n",
    "\n",
    "# Train tri4, which is LDA+MLLT+SAT, on all the (nodup) data.\n",
    "steps/align_fmllr.sh --nj 24 --cmd \"$train_cmd\" \\\n",
    "  data/train_nodev data/lang exp/tri3 exp/tri3_ali\n",
    "\n",
    "steps/train_sat.sh  --cmd \"$train_cmd\" \\\n",
    "  11500 200000 data/train_nodev data/lang exp/tri3_ali exp/tri4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iVector common starts here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "# This script is called from local/nnet3/run_tdnn.sh and local/chain/run_tdnn.sh\n",
    "# (and may eventually be called by more scripts). It contains the common feature\n",
    "# preparation and ivector-related parts of the script. See those scripts for \n",
    "# examples of usage.\n",
    "\n",
    "stage=0\n",
    "train_set=train_nodev\n",
    "dev_set=\n",
    "test_sets=\"eval1 eval2 eval3\"\n",
    "gmm=tri4\n",
    "\n",
    "nnet3_affix=\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "if [ -e data/train_dev ] ;then\n",
    "  dev_set=train_dev\n",
    "fi\n",
    "\n",
    "gmm_dir=exp/${gmm}\n",
    "ali_dir=exp/${gmm}_ali_${train_set}_sp\n",
    "\n",
    "for f in data/${train_set}/feats.scp ${gmm_dir}/final.mdl; do\n",
    "  if [ ! -f $f ]; then\n",
    "    echo \"$0: expected file $f to exist\"\n",
    "    exit 1\n",
    "  fi\n",
    "done\n",
    "\n",
    "if [ $stage -le 1 ]; then\n",
    "  # Although the nnet will be trained by high resolution data, we still have to perturb\n",
    "  # the normal data to get the alignment _sp stands for speed-perturbed\n",
    "  echo \"$0: preparing directory for low-resolution speed-perturbed data (for alignment)\"\n",
    "  utils/data/perturb_data_dir_speed_3way.sh data/${train_set} data/${train_set}_sp\n",
    "  echo \"$0: making MFCC featuresfor low-resolution speed-perturbed data\"\n",
    "  steps/make_mfcc.sh --cmd \"$train_cmd\" --nj 24 data/${train_set}_sp || exit 1;\n",
    "  steps/compute_cmvn_stats.sh data/${train_set}_sp || exit 1;\n",
    "  utils/fix_data_dir.sh data/${train_set}_sp\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "stage=0\n",
    "train_set=train_nodev\n",
    "gmm=tri4\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "gmm_dir=exp/${gmm}\n",
    "ali_dir=exp/${gmm}_ali_${train_set}_sp\n",
    "\n",
    "if [ $stage -le 2 ]; then\n",
    "  echo \"$0: aligning with the perturbed low-resolution data\"\n",
    "  steps/align_fmllr.sh --nj 24 --cmd \"$train_cmd\" \\\n",
    "    data/${train_set}_sp data/lang $gmm_dir $ali_dir || exit 1;\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "stage=0\n",
    "train_set=train_nodev\n",
    "dev_set=\n",
    "test_sets=\"eval1 eval2 eval3\"\n",
    "gmm=tri4\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "if [ $stage -le 3 ]; then\n",
    "  #Create high-resolution MFCC features (with 40 cepstra instead of 13).\n",
    "  # this shows how you can split across multiple file-systems.\n",
    "  echo \"$0: creating high-resolution MFCC features\"\n",
    "  mfccdir=data/${train_set}_sp_hires/data\n",
    "\n",
    "  for datadir in ${train_set}_sp; do\n",
    "    utils/copy_data_dir.sh data/$datadir data/${datadir}_hires\n",
    "  done\n",
    "\n",
    "  # do volume-perturbation on the training data prior to extracting hires\n",
    "  # features; this helps make trained nnets more invariant to test data volume\n",
    "  utils/data/perturb_data_dir_volume.sh data/${train_set}_sp_hires || exit 1;\n",
    "\n",
    "  # generate high-resolution MFCC feautres\n",
    "  for datadir in ${train_set}_sp; do\n",
    "    steps/make_mfcc.sh --nj 24 --mfcc-config conf/mfcc_hires.conf \\\n",
    "      --cmd \"$train_cmd\" data/${datadir}_hires || exit 1;\n",
    "    steps/compute_cmvn_stats.sh data/${datadir}_hires || exit 1;\n",
    "    utils/fix_data_dir.sh data/${datadir}_hires || exit 1;\n",
    "  done\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "stage=0\n",
    "train_set=train_nodev\n",
    "dev_set=\n",
    "test_sets=\"eval1 eval2 eval3\"\n",
    "gmm=tri4\n",
    "\n",
    "nnet3_affix=\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "if [ $stage -le 4 ]; then\n",
    "  echo \"$0: train the diagonal UBM.\"\n",
    "  # There's no data, so use all\n",
    "  mkdir -p exp/nnet3${nnet3_affix}/diag_ubm\n",
    "  temp_data_root=exp/nnet3${nnet3_affix}/diag_ubm\n",
    "\n",
    "  num_utts_total=$(wc -l <data/${train_set}_sp_hires/utt2spk)\n",
    "  num_utts=$[$num_utts_total/1]\n",
    "  utils/data/subset_data_dir.sh data/${train_set}_sp_hires \\\n",
    "    $num_utts ${temp_data_root}/${train_set}_sp_hires_subset\n",
    "\n",
    "  echo \"$0: computing a PCA transform from the hires data.\"\n",
    "  steps/online/nnet2/get_pca_transform.sh --cmd \"$train_cmd\" \\\n",
    "    --splice-opts \"--left-context=4 --right-context=4\" \\\n",
    "    --max-utts 10000 --subsample 2 \\\n",
    "    ${temp_data_root}/${train_set}_sp_hires_subset \\\n",
    "    exp/nnet3${nnet3_affix}/pca_transform\n",
    "\n",
    "  echo \"$0: training the diagonal UBM.\"\n",
    "  # Use 512 Gaussians in the UBM.\n",
    "  steps/online/nnet2/train_diag_ubm.sh --cmd \"$train_cmd\" --nj 24 \\\n",
    "    --num-frames 500000 --num-threads 8 \\\n",
    "    ${temp_data_root}/${train_set}_sp_hires_subset 512 \\\n",
    "    exp/nnet3${nnet3_affix}/pca_transform exp/nnet3${nnet3_affix}/diag_ubm\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "stage=0\n",
    "train_set=train_nodev\n",
    "dev_set=\n",
    "test_sets=\"eval1 eval2 eval3\"\n",
    "gmm=tri4\n",
    "\n",
    "nnet3_affix=\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "if [ $stage -le 5 ]; then\n",
    "  # Train the iVector extractor.Use all of the speed-perturbed data since iVector extractors\n",
    "  # can be sensitive to the amount of data. The script defaults to an iVector dimension of 100\n",
    "  # even though $nj is just 2 (10 on high end set ups), each job uses multiple processes and threads.\n",
    "  echo \"$0: training the iVector extractor\"\n",
    "  steps/online/nnet2/train_ivector_extractor.sh --cmd \"$train_cmd\" --nj 2 \\\n",
    "    data/${train_set}_sp_hires exp/nnet3${nnet3_affix}/diag_ubm \\\n",
    "    exp/nnet3${nnet3_affix}/extractor || exit 1;\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "stage=0\n",
    "train_set=train_nodev\n",
    "dev_set=\n",
    "test_sets=\"eval1 eval2 eval3\"\n",
    "gmm=tri4\n",
    "\n",
    "nnet3_affix=\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "if [ $stage -le 6 ]; then\n",
    "  # We extract iVectors on the speed-perturbed training data after combining\n",
    "  # short segments, which will be what we train the system on. With --utts-per-spk-max 2,\n",
    "  # the scripts pairs the utterances into twos, and treats each of these pairs\n",
    "  # as one speaker; this gives more diversity in iVectors.\n",
    "  # Note that these are extracted 'online'.\n",
    "\n",
    "  # Note, we don't encode the 'max2' in the name of the iVectordir even though\n",
    "  # that's the data we extract the iVectors from, as it's still going to be \n",
    "  # valid for the non-'max2' data, the utterance list is the same.\n",
    "  ivectordir=exp/nnet3${nnet3_affix}/ivectors_${train_set}_sp_hires\n",
    "\n",
    "  # having a larger number of speakers is helpful for generalization, and to\n",
    "  # handle per-utterance decoding well (iVector starts at zero).\n",
    "  temp_data_root=${ivectordir}\n",
    "  utils/data/modify_speaker_info.sh --utts-per-spk-max 2 \\\n",
    "    data/${train_set}_sp_hires ${temp_data_root}/${train_set}_sp_hires_max2\n",
    "\n",
    "  steps/online/nnet2/extract_ivectors_online.sh --cmd \"$train_cmd\" --nj 24 \\\n",
    "    ${temp_data_root}/${train_set}_sp_hires_max2 \\\n",
    "    exp/nnet3${nnet3_affix}/extractor $ivectordir\n",
    "\n",
    "  # Also extract iVectors for the test data, but in this case we don't need the speed\n",
    "  # perturbation (sp).\n",
    "  #for datadir in $dev_set $test_sets; do\n",
    "  #  steps/online/nnet2/extract_ivectors_online.sh --cmd \"$train_cmd\" --nj 10 \\\n",
    "  #    data/${datadir}_hires exp/nnet3${nnet3_affix}/extractor \\\n",
    "  #    exp/nnet3${nnet3_affix}/ivectors_${datadir}_hires\n",
    "  #done\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural net training starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "# First the options that are passed through to run_ivector_common.sh\n",
    "# (some of which are also used in this script directly).\n",
    "gmm=tri4\n",
    "train_set=train_nodev\n",
    "nnet3_affix=\n",
    "# The rest are configs specific to this script. Most of the parameters\n",
    "# are just hardcoded at this level, in the commands below.\n",
    "affix=7n  # affix for the TDNN directory name\n",
    "tree_affix=\n",
    "\n",
    "# End configuration section.\n",
    "echo \"$0 $@\"  # Print the command line for logging\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "if ! cuda-compiled; then\n",
    "  cat <<EOF && exit 1\n",
    "This script is intended to be used with GPUs but you have not compiled Kaldi with CUDA\n",
    "If you want to use GPUs (and have them), go to src/, and configure and make on a machine\n",
    "where \"nvcc\" is installed.\n",
    "EOF\n",
    "fi\n",
    "\n",
    "\n",
    "gmm_dir=exp/$gmm\n",
    "ali_dir=exp/${gmm}_ali_${train_set}_sp\n",
    "tree_dir=exp/chain${nnet3_affix}/tree${tree_affix:+_$tree_affix}\n",
    "lang=data/lang_chain\n",
    "lat_dir=exp/chain${nnet3_affix}/${gmm}_${train_set}_sp_lats\n",
    "dir=exp/chain${nnet3_affix}/tdnn${affix}\n",
    "train_data_dir=data/${train_set}_sp_hires\n",
    "lores_train_data_dir=data/${train_set}_sp\n",
    "train_ivector_dir=exp/nnet3${nnet3_affix}/ivectors_${train_set}_sp_hires\n",
    "\n",
    "for fa in $gmm_dir/final.mdl $train_data_dir/feats.scp $train_ivector_dir/ivector_online.scp \\\n",
    "  $lores_train_data_dir/feats.scp $ali_dir/ali.1.gz; do\n",
    "  # unfortunately, this check won't work on jupyter's bash magic\n",
    "  #[ ! -f $fa ] && echo \"$0; expected file $fa to exist\" && exit 1;\n",
    "  ls -ltrh $fa\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "# First the options that are passed through to run_ivector_common.sh\n",
    "# (some of which are also used in this script directly).\n",
    "stage=0\n",
    "gmm=tri4\n",
    "train_set=train_nodev\n",
    "nnet3_affix=\n",
    "# The rest are configs specific to this script. Most of the parameters\n",
    "# are just hardcoded at this level, in the commands below.\n",
    "affix=7n  # affix for the TDNN directory name\n",
    "tree_affix=\n",
    "\n",
    "# End configuration section.\n",
    "echo \"$0 $@\"  # Print the command line for logging\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "if ! cuda-compiled; then\n",
    "  cat <<EOF && exit 1\n",
    "This script is intended to be used with GPUs but you have not compiled Kaldi with CUDA\n",
    "If you want to use GPUs (and have them), go to src/, and configure and make on a machine\n",
    "where \"nvcc\" is installed.\n",
    "EOF\n",
    "fi\n",
    "\n",
    "\n",
    "gmm_dir=exp/$gmm\n",
    "ali_dir=exp/${gmm}_ali_${train_set}_sp\n",
    "tree_dir=exp/chain${nnet3_affix}/tree${tree_affix:+_$tree_affix}\n",
    "lang=data/lang_chain\n",
    "lat_dir=exp/chain${nnet3_affix}/${gmm}_${train_set}_sp_lats\n",
    "dir=exp/chain${nnet3_affix}/tdnn${affix}\n",
    "train_data_dir=data/${train_set}_sp_hires\n",
    "lores_train_data_dir=data/${train_set}_sp\n",
    "train_ivector_dir=exp/nnet3${nnet3_affix}/ivectors_${train_set}_sp_hires\n",
    "\n",
    "if [ $stage -le 9 ]; then\n",
    "  # Get the alignments as lattices (gives the LF-MMI training more freedom).\n",
    "  # use the same num-jobs as the alignments\n",
    "  steps/align_fmllr_lats.sh --nj 24 --cmd \"$train_cmd\" ${lores_train_data_dir} \\\n",
    "    data/lang $gmm_dir $lat_dir\n",
    "  rm $lat_dir/fsts.*.gz # save space\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "# First the options that are passed through to run_ivector_common.sh\n",
    "# (some of which are also used in this script directly).\n",
    "stage=0\n",
    "\n",
    "# End configuration section.\n",
    "echo \"$0 $@\"  # Print the command line for logging\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "if ! cuda-compiled; then\n",
    "  cat <<EOF && exit 1\n",
    "This script is intended to be used with GPUs but you have not compiled Kaldi with CUDA\n",
    "If you want to use GPUs (and have them), go to src/, and configure and make on a machine\n",
    "where \"nvcc\" is installed.\n",
    "EOF\n",
    "fi\n",
    "\n",
    "lang=data/lang_chain\n",
    "\n",
    "if [ $stage -le 10 ]; then\n",
    "  # Create a version of the lang/ directory that has one state per phone in the\n",
    "  # topo file. [note, it really has two states.. the first one is only repeated\n",
    "  # once, the second one has zero or more repeats.]\n",
    "  rm -rf $lang\n",
    "  cp -r data/lang $lang\n",
    "  silphonelist=$(cat $lang/phones/silence.csl) || exit 1;\n",
    "  nonsilphonelist=$(cat $lang/phones/nonsilence.csl) || exit 1;\n",
    "  # Use our special topology... note that later on may have to tune this\n",
    "  # topology.\n",
    "  steps/nnet3/chain/gen_topo.py $nonsilphonelist $silphonelist >$lang/topo\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "# First the options that are passed through to run_ivector_common.sh\n",
    "# (some of which are also used in this script directly).\n",
    "stage=0\n",
    "decode_nj=10\n",
    "train_set=train_nodev\n",
    "dev_set=\n",
    "test_sets=\"eval1 eval2 eval3\"\n",
    "gmm=tri4\n",
    "nnet3_affix=\n",
    "\n",
    "# The rest are configs specific to this script. Most of the parameters\n",
    "# are just hardcoded at this level, in the commands below.\n",
    "affix=7n  # affix for the TDNN directory name\n",
    "tree_affix=\n",
    "train_stage=-10\n",
    "get_egs_stage=-10\n",
    "decode_iter=\n",
    "\n",
    "# training options\n",
    "# training chunk-options\n",
    "decode_iter=\n",
    "num_epochs=10\n",
    "initial_effective_lrate=0.001\n",
    "final_effective_lrate=0.0001\n",
    "leftmost_questions_truncate=-1\n",
    "max_param_change=2.0\n",
    "final_layer_normalize_target=0.5\n",
    "num_jobs_initial=1\n",
    "num_jobs_final=3\n",
    "minibatch_size=128,64\n",
    "#frames_per_eg=150,140,100\n",
    "frames_per_eg=75,70,50\n",
    "remove_egs=false\n",
    "common_egs_dir=\n",
    "xent_regularize=0.1\n",
    "\n",
    "test_online_decoding=false  # if true, it will run the last decoding stage.\n",
    "\n",
    "# End configuration section.\n",
    "echo \"$0 $@\"  # Print the command line for logging\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "gmm_dir=exp/$gmm\n",
    "ali_dir=exp/${gmm}_ali_${train_set}_sp\n",
    "tree_dir=exp/chain${nnet3_affix}/tree${tree_affix:+_$tree_affix}\n",
    "lang=data/lang_chain\n",
    "lat_dir=exp/chain${nnet3_affix}/${gmm}_${train_set}_sp_lats\n",
    "dir=exp/chain${nnet3_affix}/tdnn${affix}\n",
    "train_data_dir=data/${train_set}_sp_hires\n",
    "lores_train_data_dir=data/${train_set}_sp\n",
    "train_ivector_dir=exp/nnet3${nnet3_affix}/ivectors_${train_set}_sp_hires\n",
    "\n",
    "if [ $stage -le 11 ]; then\n",
    "  # Build a tree using our new topology. This is the critically different\n",
    "  # step compared with other recipes.\n",
    "  steps/nnet3/chain/build_tree.sh --frame-subsampling-factor 3 \\\n",
    "      --leftmost-questions-truncate $leftmost_questions_truncate \\\n",
    "      --context-opts \"--context-width=2 --central-position=1\" \\\n",
    "      --cmd \"$train_cmd\" 7000 ${lores_train_data_dir} $lang $ali_dir $tree_dir\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "# First the options that are passed through to run_ivector_common.sh\n",
    "# (some of which are also used in this script directly).\n",
    "stage=0\n",
    "decode_nj=10\n",
    "train_set=train_nodev\n",
    "dev_set=\n",
    "test_sets=\"eval1 eval2 eval3\"\n",
    "gmm=tri4\n",
    "nnet3_affix=\n",
    "\n",
    "# The rest are configs specific to this script. Most of the parameters\n",
    "# are just hardcoded at this level, in the commands below.\n",
    "affix=7n  # affix for the TDNN directory name\n",
    "tree_affix=\n",
    "train_stage=-10\n",
    "get_egs_stage=-10\n",
    "decode_iter=\n",
    "\n",
    "# training options\n",
    "# training chunk-options\n",
    "decode_iter=\n",
    "num_epochs=10\n",
    "initial_effective_lrate=0.001\n",
    "final_effective_lrate=0.0001\n",
    "leftmost_questions_truncate=-1\n",
    "max_param_change=2.0\n",
    "final_layer_normalize_target=0.5\n",
    "num_jobs_initial=1\n",
    "num_jobs_final=3\n",
    "minibatch_size=128,64\n",
    "#frames_per_eg=150,140,100\n",
    "frames_per_eg=75,70,50\n",
    "remove_egs=false\n",
    "common_egs_dir=\n",
    "xent_regularize=0.1\n",
    "\n",
    "test_online_decoding=false  # if true, it will run the last decoding stage.\n",
    "\n",
    "# End configuration section.\n",
    "echo \"$0 $@\"  # Print the command line for logging\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "gmm_dir=exp/$gmm\n",
    "ali_dir=exp/${gmm}_ali_${train_set}_sp\n",
    "tree_dir=exp/chain${nnet3_affix}/tree${tree_affix:+_$tree_affix}\n",
    "lang=data/lang_chain\n",
    "lat_dir=exp/chain${nnet3_affix}/${gmm}_${train_set}_sp_lats\n",
    "dir=exp/chain${nnet3_affix}/tdnn${affix}\n",
    "train_data_dir=data/${train_set}_sp_hires\n",
    "lores_train_data_dir=data/${train_set}_sp\n",
    "train_ivector_dir=exp/nnet3${nnet3_affix}/ivectors_${train_set}_sp_hires\n",
    "\n",
    "if [ $stage -le 12 ]; then\n",
    "  echo \"$0: creating neural net configs using the xconfig parser\";\n",
    "\n",
    "  num_targets=$(tree-info $tree_dir/tree |grep num-pdfs|awk '{print $2}')\n",
    "  learning_rate_factor=$(echo \"print 0.5/$xent_regularize\" | python2)\n",
    "  opts=\"l2-regularize=0.002\"\n",
    "  linear_opts=\"orthonormal-constraint=1.0\"\n",
    "  output_opts=\"l2-regularize=0.0005 bottleneck-dim=256\"\n",
    "\n",
    "  mkdir -p $dir/configs\n",
    "\n",
    "  cat <<EOF > $dir/configs/network.xconfig\n",
    "  input dim=100 name=ivector\n",
    "  input dim=40 name=input\n",
    "\n",
    "  # please note that it is important to have input layer with the name=input\n",
    "  # as the layer immediately preceding the fixed-affine-layer to enable\n",
    "  # the use of short notation for the descriptor\n",
    "  fixed-affine-layer name=lda input=Append(-1,0,1,ReplaceIndex(ivector, t, 0)) affine-transform-file=$dir/configs/lda.mat\n",
    "\n",
    "  # the first splicing is moved before the lda layer, so no splicing here\n",
    "  relu-batchnorm-layer name=tdnn1 $opts dim=1280\n",
    "  linear-component name=tdnn2l dim=256 $linear_opts input=Append(-1,0)\n",
    "  relu-batchnorm-layer name=tdnn2 $opts input=Append(0,1) dim=1280\n",
    "  linear-component name=tdnn3l dim=256 $linear_opts\n",
    "  relu-batchnorm-layer name=tdnn3 $opts dim=1280\n",
    "  linear-component name=tdnn4l dim=256 $linear_opts input=Append(-1,0)\n",
    "  relu-batchnorm-layer name=tdnn4 $opts input=Append(0,1) dim=1280\n",
    "  linear-component name=tdnn5l dim=256 $linear_opts\n",
    "  relu-batchnorm-layer name=tdnn5 $opts dim=1280 input=Append(tdnn5l, tdnn3l)\n",
    "  linear-component name=tdnn6l dim=256 $linear_opts input=Append(-3,0)\n",
    "  relu-batchnorm-layer name=tdnn6 $opts input=Append(0,3) dim=1280\n",
    "  linear-component name=tdnn7l dim=256 $linear_opts input=Append(-3,0)\n",
    "  relu-batchnorm-layer name=tdnn7 $opts input=Append(0,3,tdnn6l,tdnn4l,tdnn2l) dim=1280\n",
    "  linear-component name=tdnn8l dim=256 $linear_opts input=Append(-3,0)\n",
    "  relu-batchnorm-layer name=tdnn8 $opts input=Append(0,3) dim=1280\n",
    "  linear-component name=tdnn9l dim=256 $linear_opts input=Append(-3,0)\n",
    "  relu-batchnorm-layer name=tdnn9 $opts input=Append(0,3,tdnn8l,tdnn6l,tdnn4l) dim=1280\n",
    "  linear-component name=tdnn10l dim=256 $linear_opts input=Append(-3,0)\n",
    "  relu-batchnorm-layer name=tdnn10 $opts input=Append(0,3) dim=1280\n",
    "  linear-component name=tdnn11l dim=256 $linear_opts input=Append(-3,0)\n",
    "  relu-batchnorm-layer name=tdnn11 $opts input=Append(0,3,tdnn10l,tdnn8l,tdnn6l) dim=1280\n",
    "  linear-component name=prefinal-l dim=256 $linear_opts\n",
    "\n",
    "  relu-batchnorm-layer name=prefinal-chain input=prefinal-l $opts dim=1280\n",
    "  output-layer name=output include-log-softmax=false dim=$num_targets $output_opts\n",
    "\n",
    "  relu-batchnorm-layer name=prefinal-xent input=prefinal-l $opts dim=1280\n",
    "  output-layer name=output-xent dim=$num_targets learning-rate-factor=$learning_rate_factor $output_opts\n",
    "EOF\n",
    "  steps/nnet3/xconfig_to_configs.py --xconfig-file $dir/configs/network.xconfig --config-dir $dir/configs/\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "# First the options that are passed through to run_ivector_common.sh\n",
    "# (some of which are also used in this script directly).\n",
    "stage=0\n",
    "decode_nj=10\n",
    "train_set=train_nodev\n",
    "dev_set=\n",
    "test_sets=\"eval1 eval2 eval3\"\n",
    "gmm=tri4\n",
    "nnet3_affix=\n",
    "\n",
    "# The rest are configs specific to this script. Most of the parameters\n",
    "# are just hardcoded at this level, in the commands below.\n",
    "affix=7n  # affix for the TDNN directory name\n",
    "tree_affix=\n",
    "train_stage=3330\n",
    "get_egs_stage=-10\n",
    "decode_iter=\n",
    "\n",
    "# training options\n",
    "# training chunk-options\n",
    "decode_iter=\n",
    "num_epochs=10\n",
    "initial_effective_lrate=0.001\n",
    "final_effective_lrate=0.0001\n",
    "leftmost_questions_truncate=-1\n",
    "max_param_change=2.0\n",
    "final_layer_normalize_target=0.5\n",
    "num_jobs_initial=1\n",
    "num_jobs_final=1\n",
    "minibatch_size=128,64\n",
    "#frames_per_eg=150,140,100\n",
    "frames_per_eg=75,70,50\n",
    "remove_egs=false\n",
    "common_egs_dir=\n",
    "xent_regularize=0.1\n",
    "\n",
    "test_online_decoding=false  # if true, it will run the last decoding stage.\n",
    "\n",
    "# End configuration section.\n",
    "echo \"$0 $@\"  # Print the command line for logging\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "gmm_dir=exp/$gmm\n",
    "ali_dir=exp/${gmm}_ali_${train_set}_sp\n",
    "tree_dir=exp/chain${nnet3_affix}/tree${tree_affix:+_$tree_affix}\n",
    "lang=data/lang_chain\n",
    "lat_dir=exp/chain${nnet3_affix}/${gmm}_${train_set}_sp_lats\n",
    "dir=exp/chain${nnet3_affix}/tdnn${affix}\n",
    "train_data_dir=data/${train_set}_sp_hires\n",
    "lores_train_data_dir=data/${train_set}_sp\n",
    "train_ivector_dir=exp/nnet3${nnet3_affix}/ivectors_${train_set}_sp_hires\n",
    "\n",
    "if [ $stage -le 13 ]; then\n",
    "  steps/nnet3/chain/train.py --stage $train_stage \\\n",
    "    --cmd \"$train_cmd\" \\\n",
    "    --feat.online-ivector-dir $train_ivector_dir \\\n",
    "    --feat.cmvn-opts \"--norm-means=false --norm-vars=false\" \\\n",
    "    --chain.xent-regularize $xent_regularize \\\n",
    "    --chain.leaky-hmm-coefficient 0.1 \\\n",
    "    --chain.l2-regularize 0.0 \\\n",
    "    --chain.apply-deriv-weights false \\\n",
    "    --chain.lm-opts=\"--num-extra-lm-states=2000\" \\\n",
    "    --egs.dir \"$common_egs_dir\" \\\n",
    "    --egs.stage $get_egs_stage \\\n",
    "    --egs.opts \"--frames-overlap-per-eg 0\" \\\n",
    "    --egs.chunk-width $frames_per_eg \\\n",
    "    --trainer.num-chunk-per-minibatch 128 \\\n",
    "    --trainer.frames-per-iter 1500000 \\\n",
    "    --trainer.num-epochs $num_epochs \\\n",
    "    --trainer.optimization.num-jobs-initial $num_jobs_initial \\\n",
    "    --trainer.optimization.num-jobs-final $num_jobs_final \\\n",
    "    --trainer.optimization.initial-effective-lrate 0.001 \\\n",
    "    --trainer.optimization.final-effective-lrate 0.0001 \\\n",
    "    --trainer.max-param-change 2.0 \\\n",
    "    --use-gpu=wait \\\n",
    "    --cleanup.remove-egs $remove_egs \\\n",
    "    --feat-dir $train_data_dir \\\n",
    "    --tree-dir $tree_dir \\\n",
    "    --lat-dir $lat_dir \\\n",
    "    --dir $dir  || exit 1;\n",
    "\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "lmDir=data/exp/lm\n",
    "srcdict=data/exp/dict_bigtext/lexicon.txt\n",
    "dir=data/exp/dict_bigtext\n",
    "\n",
    "export LC_ALL=C\n",
    "\n",
    "cat data/exp/dict_bigtext/ALL_dict | sed '/<sp>/d; /<unk>/d; /^[ ]*$/d' | sort --parallel=8 | uniq > $srcdict\n",
    "\n",
    "\n",
    "cat $srcdict > $dir/lexicon1.txt || exit 1;\n",
    "\n",
    "#cat $dir/lexicon1.txt | awk '{ for(n=2;n<=NF;n++){ phones[$n] = 1; }} END{for (p in phones) print p;}' | \\\n",
    "#  grep -v SIL > $dir/nonsilence_phones.txt  || exit 1;\n",
    "cp data/local/dict/nonsilence_phones.txt $dir/nonsilence_phones.txt\n",
    "\n",
    "#( echo sil; echo spn; echo nsn; echo lau ) > $dir/silence_phones.txt\n",
    "#( echo SIL ; echo SPN ; ) > $dir/silence_phones.txt\n",
    "cp data/local/dict/silence_phones.txt $dir/silence_phones.txt\n",
    "\n",
    "#echo SIL > $dir/optional_silence.txt\n",
    "cp data/local/dict/optional_silence.txt $dir/optional_silence.txt\n",
    "\n",
    "# No \"extra questions\" in the input to this setup, as we don't\n",
    "# have stress or tone.\n",
    "#echo -n >$dir/extra_questions.txt\n",
    "cp data/local/dict/extra_questions.txt $dir/extra_questions.txt\n",
    "\n",
    "# Add to the lexicon the silences, noises etc.\n",
    "( echo '<sp> SIL' ; echo '<unk> SPN'; ) | cat - $dir/lexicon1.txt  > $dir/lexicon2.txt || exit 1;\n",
    "\n",
    "cp $dir/lexicon2.txt $dir/lexicon.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    "set -e # exit on error\n",
    "\n",
    "rm -fr data/exp/lang_bigtext\n",
    "utils/prepare_lang.sh --num-sil-states 4 data/exp/dict_bigtext \"<unk>\" data/exp/lang_bigtext/tmp data/exp/lang_bigtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    "set -e # exit on error\n",
    "\n",
    "dir=data/exp/lm\n",
    "text=data/TEXT/EVERYTHING_UNIQ\n",
    "lexicon=data/exp/dict_bigtext/lexicon.txt\n",
    "mkdir -p $dir\n",
    "export LC_ALL=C \n",
    "\n",
    "cat $text | gzip -c > $dir/train.all.gz\n",
    "#cut -d' ' -f2- $text | tail -n +$heldout_sent | gzip -c > $dir/train.gz\n",
    "#cut -d' ' -f2- $text | head -n $heldout_sent > $dir/heldout\n",
    "\n",
    "cut -d' ' -f1 $lexicon > $dir/wordlist\n",
    "\n",
    "ngram-count -text $dir/train.all.gz -order 4 -limit-vocab -vocab $dir/wordlist \\\n",
    "  -unk -map-unk \"<unk>\" -kndiscount -interpolate -lm $dir/fph.o4g.kn.gz\n",
    "#echo \"PPL for CSJ LM:\"\n",
    "#ngram -unk -lm $dir/csj.o3g.kn.gz -ppl $dir/heldout\n",
    "#ngram -unk -lm $dir/csj.o3g.kn.gz -ppl $dir/heldout -debug 2 >& $dir/ppl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    "set -e # exit on error\n",
    "\n",
    "# We don't really need all these options for SRILM, since the LM training script\n",
    "# does some of the same processing (e.g. -subset -tolower)\n",
    "srilm_opts=\"-subset -prune-lowprobs -unk -tolower -order 4\"\n",
    "LM=data/exp/lm/fph.o4g.kn.gz\n",
    "rm -fr data/exp/lang_nosilp_fph_4g\n",
    "utils/format_lm_sri.sh --srilm-opts \"$srilm_opts\" \\\n",
    "  data/exp/lang_bigtext $LM data/exp/dict_bigtext/lexicon.txt data/exp/lang_nosilp_fph_4g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "# First the options that are passed through to run_ivector_common.sh\n",
    "# (some of which are also used in this script directly).\n",
    "stage=0\n",
    "decode_nj=10\n",
    "train_set=train_nodev\n",
    "dev_set=\n",
    "test_sets=\"eval1 eval2 eval3\"\n",
    "gmm=tri4\n",
    "nnet3_affix=\n",
    "\n",
    "# The rest are configs specific to this script. Most of the parameters\n",
    "# are just hardcoded at this level, in the commands below.\n",
    "affix=7n  # affix for the TDNN directory name\n",
    "tree_affix=\n",
    "train_stage=-10\n",
    "get_egs_stage=-10\n",
    "decode_iter=\n",
    "\n",
    "# training options\n",
    "# training chunk-options\n",
    "decode_iter=\n",
    "num_epochs=10\n",
    "initial_effective_lrate=0.001\n",
    "final_effective_lrate=0.0001\n",
    "leftmost_questions_truncate=-1\n",
    "max_param_change=2.0\n",
    "final_layer_normalize_target=0.5\n",
    "num_jobs_initial=1\n",
    "num_jobs_final=3\n",
    "minibatch_size=128,64\n",
    "#frames_per_eg=150,140,100\n",
    "frames_per_eg=75,70,50\n",
    "remove_egs=false\n",
    "common_egs_dir=\n",
    "xent_regularize=0.1\n",
    "\n",
    "test_online_decoding=false  # if true, it will run the last decoding stage.\n",
    "\n",
    "# End configuration section.\n",
    "echo \"$0 $@\"  # Print the command line for logging\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "gmm_dir=exp/$gmm\n",
    "ali_dir=exp/${gmm}_ali_${train_set}_sp\n",
    "tree_dir=exp/chain${nnet3_affix}/tree${tree_affix:+_$tree_affix}\n",
    "lang=data/lang_chain\n",
    "lat_dir=exp/chain${nnet3_affix}/${gmm}_${train_set}_sp_lats\n",
    "dir=exp/chain${nnet3_affix}/tdnn${affix}\n",
    "train_data_dir=data/${train_set}_sp_hires\n",
    "lores_train_data_dir=data/${train_set}_sp\n",
    "train_ivector_dir=exp/nnet3${nnet3_affix}/ivectors_${train_set}_sp_hires\n",
    "\n",
    "if [ $stage -le 14 ]; then\n",
    "  # Note: it might appear that this $lang directory is mismatched, and it is as\n",
    "  # far as the 'topo' is concerned, but this script doesn't read the 'topo' from\n",
    "  # the lang directory.\n",
    "  utils/mkgraph.sh --self-loop-scale 1.0 data/exp/lang_nosilp_fph_4g $dir $dir/graph_fph_4g\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
