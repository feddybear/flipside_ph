{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation\n",
    "\n",
    "### Speed perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "if [ -e data/train_dev ]; then\n",
    "  dev_set=train_dev\n",
    "fi\n",
    "\n",
    "stage=0\n",
    "clean_train_set=train_nodev\n",
    "tempo_aug_set=${clean_train_set}_tp\n",
    "\n",
    "train_set=\n",
    "dev_set=\n",
    "clean_test_sets=\"\"\n",
    "test_sets=\"\"\n",
    "gmm=tri4\n",
    "\n",
    "gmm_dir=exp/${gmm}\n",
    "ali_dir=exp/${gmm}_ali_${clean_train_set}_tp\n",
    "\n",
    "for f in data/${clean_train_set}/feats.scp ${gmm_dir}/final.mdl; do\n",
    "  if [ ! -f $f ]; then\n",
    "    echo \"$0: expected file $f to exist\"\n",
    "    exit 1\n",
    "  fi\n",
    "done\n",
    "\n",
    "if [ $stage -le 1 ]; then\n",
    "  # Although the nnet will be trained by high resolution data, we still have to perturb\n",
    "  # the normal data to get the alignment _tp stands for tempo-perturbed\n",
    "  echo \"$0: preparing directory for low-resolution tempo-perturbed data (for alignment)\"\n",
    "  \n",
    "  local/fph_perturb_data_dir_tempo_4way.sh --always-include-prefix true \\\n",
    "    data/${clean_train_set} data/${tempo_aug_set}\n",
    "\n",
    "  steps/make_mfcc.sh --write-utt2num-frames true --cmd \"$train_cmd\" --nj 24 data/${tempo_aug_set} || exit 1;\n",
    "  steps/compute_cmvn_stats.sh data/${tempo_aug_set} || exit 1;\n",
    "  utils/fix_data_dir.sh data/${tempo_aug_set}\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverberation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "if [ -e data/train_dev ]; then\n",
    "  dev_set=train_dev\n",
    "fi\n",
    "\n",
    "stage=0\n",
    "clean_train_set=train_nodev\n",
    "tempo_aug_set=${clean_train_set}_tp\n",
    "rvb_tempo_aug_set=${tempo_aug_set}_rvb\n",
    "train_set=${clean_train_set}_tp_aug\n",
    "dev_set=\n",
    "clean_test_sets=\"\"\n",
    "test_sets=\"\"\n",
    "gmm=tri4\n",
    "\n",
    "gmm_dir=exp/${gmm}\n",
    "ali_dir=exp/${gmm}_ali_${clean_train_set}_tp\n",
    "\n",
    "for f in data/${clean_train_set}/feats.scp ${gmm_dir}/final.mdl; do\n",
    "  if [ ! -f $f ]; then\n",
    "    echo \"$0: expected file $f to exist\"\n",
    "    exit 1\n",
    "  fi\n",
    "done\n",
    "\n",
    "if [ $stage -le 2 ]; then\n",
    "  frame_shift=0.01\n",
    "  ## version 1 for recordings already segmented by utterance (NO NEED FOR MIXED)\n",
    "  #awk -v frame_shift=$frame_shift '{print $1, $2*frame_shift;} END {}' data/${clean_train_set}_tp/utt2num_frames \\\n",
    "  #  > data/${clean_train_set}_tp/reco2dur\n",
    "\n",
    "  rvb_opts=()\n",
    "  rvb_opts+=(--rir-set-parameters \"0.5, RIRS_NOISES/simulated_rirs/smallroom/rir_list\")\n",
    "  rvb_opts+=(--rir-set-parameters \"0.5, RIRS_NOISES/simulated_rirs/mediumroom/rir_list\")\n",
    "  rvb_opts+=(--noise-set-parameters \"RIRS_NOISES/pointsource_noises/noise_list\")\n",
    "  \n",
    "  steps/data/reverberate_data_dir.py \\\n",
    "    \"${rvb_opts[@]}\" \\\n",
    "    --prefix \"reverb\" \\\n",
    "    --foreground-snrs \"20:10:15:5:0\" \\\n",
    "    --background-snrs \"20:10:15:5:0\" \\\n",
    "    --speech-rvb-probability 1 \\\n",
    "    --pointsource-noise-addition-probability 1 \\\n",
    "    --isotropic-noise-addition-probability 1 \\\n",
    "    --num-replications 1 \\\n",
    "    --max-noises-per-minute 1 \\\n",
    "    --source-sampling-rate 16000 \\\n",
    "    data/${tempo_aug_set} data/${rvb_tempo_aug_set}\n",
    "\n",
    "  \n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "# This script is called from local/nnet3/run_tdnn.sh and local/chain/run_tdnn.sh\n",
    "# (and may eventually be called by more scripts). It contains the common feature\n",
    "# preparation and ivector-related parts of the script. See those scripts for \n",
    "# examples of usage.\n",
    "\n",
    "stage=0\n",
    "clean_train_set=train_nodev\n",
    "tempo_aug_set=${clean_train_set}_tp\n",
    "rvb_tempo_aug_set=${tempo_aug_set}_rvb\n",
    "train_set=\n",
    "dev_set=\n",
    "clean_test_sets=\"eval1 eval2 eval3\"\n",
    "test_sets=\"noisy_eval1 noisy_eval2 noisy_eval3\"\n",
    "gmm=tri4\n",
    "\n",
    "nnet3_affix=\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "if [ -e data/train_dev ] ;then\n",
    "  dev_set=train_dev\n",
    "fi\n",
    "\n",
    "gmm_dir=exp/${gmm}\n",
    "ali_dir=exp/${gmm}_ali_${clean_train_set}_tp\n",
    "\n",
    "for f in data/${clean_train_set}/feats.scp ${gmm_dir}/final.mdl; do\n",
    "  if [ ! -f $f ]; then\n",
    "    echo \"$0: expected file $f to exist\"\n",
    "    exit 1\n",
    "  fi\n",
    "done\n",
    "\n",
    "if [ $stage -le 1 ]; then\n",
    "  # this step is only for complicated pipelines in wav.scp,\n",
    "  # we create explicit audio files containing the augments\n",
    "  cp data/${rvb_tempo_aug_set}/wav.scp data/${rvb_tempo_aug_set}/wav.scp_BACKUP\n",
    "\n",
    "  targetdir=data/raw/rvb\n",
    "  mkdir -p $targetdir\n",
    "\n",
    "  while read -r line; do\n",
    "    outfile=$(echo $line | awk -v outpath=$targetdir '{print outpath\"/\"$1\".wav\"}')\n",
    "    sem -j 32 bash <(echo $line | cut -d' ' -f2- | sed 's/|$//g') > $outfile\n",
    "    echo $line | awk -v outfile=$outfile '{print $1\" cat \"outfile\" |\"}' >> $targetdir/wav.scp\n",
    "  done < <( cat data/${rvb_tempo_aug_set}/wav.scp )\n",
    "\n",
    "  mv $targetdir/wav.scp data/${rvb_tempo_aug_set}/wav.scp\n",
    "\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "# This script is called from local/nnet3/run_tdnn.sh and local/chain/run_tdnn.sh\n",
    "# (and may eventually be called by more scripts). It contains the common feature\n",
    "# preparation and ivector-related parts of the script. See those scripts for \n",
    "# examples of usage.\n",
    "\n",
    "stage=0\n",
    "clean_train_set=train_nodev\n",
    "tempo_aug_set=${clean_train_set}_tp\n",
    "rvb_tempo_aug_set=${tempo_aug_set}_rvb\n",
    "train_set=\n",
    "dev_set=\n",
    "clean_test_sets=\"eval1 eval2 eval3\"\n",
    "test_sets=\"noisy_eval1 noisy_eval2 noisy_eval3\"\n",
    "gmm=tri4\n",
    "\n",
    "nnet3_affix=\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "if [ -e data/train_dev ] ;then\n",
    "  dev_set=train_dev\n",
    "fi\n",
    "\n",
    "gmm_dir=exp/${gmm}\n",
    "ali_dir=exp/${gmm}_ali_${clean_train_set}_tp\n",
    "\n",
    "for f in data/${clean_train_set}/feats.scp ${gmm_dir}/final.mdl; do\n",
    "  if [ ! -f $f ]; then\n",
    "    echo \"$0: expected file $f to exist\"\n",
    "    exit 1\n",
    "  fi\n",
    "done\n",
    "\n",
    "if [ $stage -le 1 ]; then\n",
    "  echo \"$0: making MFCC featuresfor low-resolution speed-then-noise-perturbed data\"\n",
    "  steps/make_mfcc.sh --cmd \"$train_cmd\" --nj 32 data/${rvb_tempo_aug_set} || exit 1;\n",
    "  steps/compute_cmvn_stats.sh data/${rvb_tempo_aug_set} || exit 1;\n",
    "  utils/fix_data_dir.sh data/${rvb_tempo_aug_set}\n",
    "    \n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MUSAN Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "# This script is called from local/nnet3/run_tdnn.sh and local/chain/run_tdnn.sh\n",
    "# (and may eventually be called by more scripts). It contains the common feature\n",
    "# preparation and ivector-related parts of the script. See those scripts for \n",
    "# examples of usage.\n",
    "\n",
    "stage=0\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "\n",
    "if [ $stage -le 1 ]; then\n",
    "  # Prepare the MUSAN corpus, which consists of music, speech, and noise\n",
    "  # suitable for augmentation.\n",
    "  musan_root=/storage06/share_data/intelligence/Speech/Corpus/Noise/musan\n",
    "  local/make_musan.sh $musan_root data\n",
    "  echo \"Finished preparing musan\"\n",
    "  echo \"\"\n",
    "\n",
    "  # Get the duration of the MUSAN recordings.  This will be used by the\n",
    "  # script augment_data_dir.py.\n",
    "  for name in speech noise music; do\n",
    "    utils/data/get_utt2dur.sh data/musan_${name}\n",
    "    mv data/musan_${name}/utt2dur data/musan_${name}/reco2dur\n",
    "  done\n",
    "  echo \"Finished getting duration of MUSAN recordings\"\n",
    "  echo \"\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps/data/augment_data_dir.py --utt-prefix noise --modify-spk-id true --fg-interval 1 --fg-snrs 15:10:5:0 --fg-noise-dir data/musan_noise data/train_nodev_tp data/train_nodev_tp_noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps/data/augment_data_dir.py:12: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import sys, random, argparse, os, imp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 12 ms, total: 20 ms\n",
      "Wall time: 3min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "# This script is called from local/nnet3/run_tdnn.sh and local/chain/run_tdnn.sh\n",
    "# (and may eventually be called by more scripts). It contains the common feature\n",
    "# preparation and ivector-related parts of the script. See those scripts for \n",
    "# examples of usage.\n",
    "\n",
    "stage=0\n",
    "clean_train_set=train_nodev\n",
    "tempo_aug_set=${clean_train_set}_tp\n",
    "noise_tempo_aug_set=${tempo_aug_set}_noise\n",
    "train_set=\n",
    "dev_set=\n",
    "clean_test_sets=\"eval1 eval2 eval3\"\n",
    "test_sets=\"noisy_eval1 noisy_eval2 noisy_eval3\"\n",
    "gmm=tri4\n",
    "\n",
    "nnet3_affix=\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "if [ -e data/train_dev ] ;then\n",
    "  dev_set=train_dev\n",
    "fi\n",
    "\n",
    "gmm_dir=exp/${gmm}\n",
    "ali_dir=exp/${gmm}_ali_${clean_train_set}_tp\n",
    "\n",
    "for f in data/${clean_train_set}/feats.scp ${gmm_dir}/final.mdl; do\n",
    "  if [ ! -f $f ]; then\n",
    "    echo \"$0: expected file $f to exist\"\n",
    "    exit 1\n",
    "  fi\n",
    "done\n",
    "\n",
    "if [ $stage -le 1 ]; then\n",
    "  # Augment with musan_noise\n",
    "  steps/data/augment_data_dir.py --utt-prefix \"noise\" --modify-spk-id true --fg-interval 1 --fg-snrs \"15:10:5:0\" \\\n",
    "    --fg-noise-dir \"data/musan_noise\" data/${tempo_aug_set} data/${noise_tempo_aug_set}\n",
    "    \n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "# This script is called from local/nnet3/run_tdnn.sh and local/chain/run_tdnn.sh\n",
    "# (and may eventually be called by more scripts). It contains the common feature\n",
    "# preparation and ivector-related parts of the script. See those scripts for \n",
    "# examples of usage.\n",
    "\n",
    "stage=0\n",
    "clean_train_set=train_nodev\n",
    "tempo_aug_set=${clean_train_set}_tp\n",
    "noise_tempo_aug_set=${tempo_aug_set}_noise\n",
    "train_set=\n",
    "dev_set=\n",
    "clean_test_sets=\"eval1 eval2 eval3\"\n",
    "test_sets=\"noisy_eval1 noisy_eval2 noisy_eval3\"\n",
    "gmm=tri4\n",
    "\n",
    "nnet3_affix=\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "if [ -e data/train_dev ] ;then\n",
    "  dev_set=train_dev\n",
    "fi\n",
    "\n",
    "gmm_dir=exp/${gmm}\n",
    "ali_dir=exp/${gmm}_ali_${clean_train_set}_tp\n",
    "\n",
    "for f in data/${clean_train_set}/feats.scp ${gmm_dir}/final.mdl; do\n",
    "  if [ ! -f $f ]; then\n",
    "    echo \"$0: expected file $f to exist\"\n",
    "    exit 1\n",
    "  fi\n",
    "done\n",
    "\n",
    "if [ $stage -le 1 ]; then\n",
    "  # Write files\n",
    "  cp data/${noise_tempo_aug_set}/wav.scp data/${noise_tempo_aug_set}/wav.scp_BACKUP\n",
    "\n",
    "  targetdir=data/raw/noise\n",
    "  \n",
    "  while read -r line; do\n",
    "    outfile=$(echo $line | awk -v outpath=$targetdir '{print outpath\"/\"$1\".wav\"}')\n",
    "    sem -j 28 bash <(echo $line | cut -d' ' -f2- | sed 's/|$//g') > $outfile\n",
    "    echo $line | awk -v outfile=$outfile '{print $1\" cat \"outfile\" |\"}' >> $targetdir/wav.scp\n",
    "  done < <( cat data/${noise_tempo_aug_set}/wav.scp )\n",
    "  \n",
    "  mv $targetdir/wav.scp data/${noise_tempo_aug_set}/wav.scp\n",
    "    \n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "# This script is called from local/nnet3/run_tdnn.sh and local/chain/run_tdnn.sh\n",
    "# (and may eventually be called by more scripts). It contains the common feature\n",
    "# preparation and ivector-related parts of the script. See those scripts for \n",
    "# examples of usage.\n",
    "\n",
    "stage=0\n",
    "clean_train_set=train_nodev\n",
    "tempo_aug_set=${clean_train_set}_tp\n",
    "noise_tempo_aug_set=${tempo_aug_set}_noise\n",
    "train_set=\n",
    "dev_set=\n",
    "clean_test_sets=\"eval1 eval2 eval3\"\n",
    "test_sets=\"noisy_eval1 noisy_eval2 noisy_eval3\"\n",
    "gmm=tri4\n",
    "\n",
    "nnet3_affix=\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "if [ -e data/train_dev ] ;then\n",
    "  dev_set=train_dev\n",
    "fi\n",
    "\n",
    "gmm_dir=exp/${gmm}\n",
    "ali_dir=exp/${gmm}_ali_${clean_train_set}_tp\n",
    "\n",
    "for f in data/${clean_train_set}/feats.scp ${gmm_dir}/final.mdl; do\n",
    "  if [ ! -f $f ]; then\n",
    "    echo \"$0: expected file $f to exist\"\n",
    "    exit 1\n",
    "  fi\n",
    "done\n",
    "\n",
    "if [ $stage -le 1 ]; then\n",
    "  echo \"$0: making MFCC featuresfor low-resolution speed-then-noise-perturbed data\"\n",
    "  steps/make_mfcc.sh --cmd \"$train_cmd\" --nj 32 data/${noise_tempo_aug_set} || exit 1;\n",
    "  steps/compute_cmvn_stats.sh data/${noise_tempo_aug_set} || exit 1;\n",
    "  utils/fix_data_dir.sh data/${noise_tempo_aug_set}\n",
    "    \n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixing music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps/data/augment_data_dir.py --utt-prefix music --modify-spk-id true --bg-snrs 25:20:15:10:5 --num-bg-noises 1 --bg-noise-dir data/musan_music data/train_nodev_tp data/train_nodev_tp_music\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps/data/augment_data_dir.py:12: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import sys, random, argparse, os, imp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\n",
      "Wall time: 3min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "# This script is called from local/nnet3/run_tdnn.sh and local/chain/run_tdnn.sh\n",
    "# (and may eventually be called by more scripts). It contains the common feature\n",
    "# preparation and ivector-related parts of the script. See those scripts for \n",
    "# examples of usage.\n",
    "\n",
    "stage=0\n",
    "clean_train_set=train_nodev\n",
    "tempo_aug_set=${clean_train_set}_tp\n",
    "music_tempo_aug_set=${tempo_aug_set}_music\n",
    "train_set=\n",
    "dev_set=\n",
    "clean_test_sets=\"eval1 eval2 eval3\"\n",
    "test_sets=\"noisy_eval1 noisy_eval2 noisy_eval3\"\n",
    "gmm=tri4\n",
    "\n",
    "nnet3_affix=\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "if [ -e data/train_dev ] ;then\n",
    "  dev_set=train_dev\n",
    "fi\n",
    "\n",
    "gmm_dir=exp/${gmm}\n",
    "ali_dir=exp/${gmm}_ali_${clean_train_set}_tp\n",
    "\n",
    "for f in data/${clean_train_set}/feats.scp ${gmm_dir}/final.mdl; do\n",
    "  if [ ! -f $f ]; then\n",
    "    echo \"$0: expected file $f to exist\"\n",
    "    exit 1\n",
    "  fi\n",
    "done\n",
    "\n",
    "if [ $stage -le 1 ]; then\n",
    "  \n",
    "  # Augment with musan_music\n",
    "  steps/data/augment_data_dir.py --utt-prefix \"music\" --modify-spk-id true --bg-snrs \"25:20:15:10:5\" \\\n",
    "    --num-bg-noises \"1\" --bg-noise-dir \"data/musan_music\" data/${tempo_aug_set} data/${music_tempo_aug_set}\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "# This script is called from local/nnet3/run_tdnn.sh and local/chain/run_tdnn.sh\n",
    "# (and may eventually be called by more scripts). It contains the common feature\n",
    "# preparation and ivector-related parts of the script. See those scripts for \n",
    "# examples of usage.\n",
    "\n",
    "stage=0\n",
    "clean_train_set=train_nodev\n",
    "tempo_aug_set=${clean_train_set}_tp\n",
    "music_tempo_aug_set=${tempo_aug_set}_music\n",
    "train_set=\n",
    "dev_set=\n",
    "clean_test_sets=\"eval1 eval2 eval3\"\n",
    "test_sets=\"noisy_eval1 noisy_eval2 noisy_eval3\"\n",
    "gmm=tri4\n",
    "\n",
    "nnet3_affix=\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "if [ -e data/train_dev ] ;then\n",
    "  dev_set=train_dev\n",
    "fi\n",
    "\n",
    "gmm_dir=exp/${gmm}\n",
    "ali_dir=exp/${gmm}_ali_${clean_train_set}_tp\n",
    "\n",
    "for f in data/${clean_train_set}/feats.scp ${gmm_dir}/final.mdl; do\n",
    "  if [ ! -f $f ]; then\n",
    "    echo \"$0: expected file $f to exist\"\n",
    "    exit 1\n",
    "  fi\n",
    "done\n",
    "\n",
    "if [ $stage -le 1 ]; then\n",
    "  echo \"$0: making MFCC featuresfor low-resolution speed-then-noise-perturbed data\"\n",
    "  steps/make_mfcc.sh --cmd \"$train_cmd\" --nj 32 data/${music_tempo_aug_set} || exit 1;\n",
    "  steps/compute_cmvn_stats.sh data/${music_tempo_aug_set} || exit 1;\n",
    "  utils/fix_data_dir.sh data/${music_tempo_aug_set}\n",
    "    \n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Babble noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps/data/augment_data_dir.py --utt-prefix babble --modify-spk-id true --bg-snrs 30:25:20:17:15 --num-bg-noises 7:8:9:10:11 --bg-noise-dir data/musan_speech data/train_nodev_tp data/train_nodev_tp_babble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps/data/augment_data_dir.py:12: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import sys, random, argparse, os, imp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 8 ms, total: 24 ms\n",
      "Wall time: 4min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "# This script is called from local/nnet3/run_tdnn.sh and local/chain/run_tdnn.sh\n",
    "# (and may eventually be called by more scripts). It contains the common feature\n",
    "# preparation and ivector-related parts of the script. See those scripts for \n",
    "# examples of usage.\n",
    "\n",
    "stage=0\n",
    "clean_train_set=train_nodev\n",
    "tempo_aug_set=${clean_train_set}_tp\n",
    "babble_tempo_aug_set=${tempo_aug_set}_babble\n",
    "train_set=\n",
    "dev_set=\n",
    "clean_test_sets=\"eval1 eval2 eval3\"\n",
    "test_sets=\"noisy_eval1 noisy_eval2 noisy_eval3\"\n",
    "gmm=tri4\n",
    "\n",
    "nnet3_affix=\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "if [ -e data/train_dev ] ;then\n",
    "  dev_set=train_dev\n",
    "fi\n",
    "\n",
    "gmm_dir=exp/${gmm}\n",
    "ali_dir=exp/${gmm}_ali_${clean_train_set}_tp\n",
    "\n",
    "for f in data/${clean_train_set}/feats.scp ${gmm_dir}/final.mdl; do\n",
    "  if [ ! -f $f ]; then\n",
    "    echo \"$0: expected file $f to exist\"\n",
    "    exit 1\n",
    "  fi\n",
    "done\n",
    "\n",
    "if [ $stage -le 1 ]; then\n",
    "  rm -fr data/${babble_tempo_aug_set}\n",
    "  # Augment with musan_speech\n",
    "  steps/data/augment_data_dir.py --utt-prefix \"babble\" --modify-spk-id true --bg-snrs \"30:25:20:17:15\" \\\n",
    "    --num-bg-noises \"7:8:9:10:11\" --bg-noise-dir \"data/musan_speech\" data/${tempo_aug_set} \\\n",
    "    data/${babble_tempo_aug_set}\n",
    "  \n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "# This script is called from local/nnet3/run_tdnn.sh and local/chain/run_tdnn.sh\n",
    "# (and may eventually be called by more scripts). It contains the common feature\n",
    "# preparation and ivector-related parts of the script. See those scripts for \n",
    "# examples of usage.\n",
    "\n",
    "stage=0\n",
    "clean_train_set=train_nodev\n",
    "train_set=\n",
    "dev_set=\n",
    "clean_test_sets=\"eval1 eval2 eval3\"\n",
    "test_sets=\"noisy_eval1 noisy_eval2 noisy_eval3\"\n",
    "gmm=tri4\n",
    "\n",
    "nnet3_affix=\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "if [ -e data/train_dev ] ;then\n",
    "  dev_set=train_dev\n",
    "fi\n",
    "\n",
    "gmm_dir=exp/${gmm}\n",
    "ali_dir=exp/${gmm}_ali_${clean_train_set}_tp\n",
    "\n",
    "for f in data/${clean_train_set}/feats.scp ${gmm_dir}/final.mdl; do\n",
    "  if [ ! -f $f ]; then\n",
    "    echo \"$0: expected file $f to exist\"\n",
    "    exit 1\n",
    "  fi\n",
    "done\n",
    "\n",
    "if [ $stage -le 1 ]; then\n",
    "  # Augment with musan_noise\n",
    "  steps/data/augment_data_dir.py --utt-suffix \"noise\" --fg-interval 1 --fg-snrs \"15:10:5:0\" \\\n",
    "    --fg-noise-dir \"data/musan_noise\" data/${clean_train_set}_tp data/${clean_train_set}_tp_noise\n",
    "  \n",
    "  # Augment with musan_music\n",
    "  steps/data/augment_data_dir.py --utt-suffix \"music\" --bg-snrs \"15:10:8:5\" --num-bg-noises \"1\" \\\n",
    "    --bg-noise-dir \"data/musan_music\" data/${clean_train_set}_tp data/${clean_train_set}_tp_music\n",
    "  \n",
    "  # Augment with musan_speech\n",
    "  steps/data/augment_data_dir.py --utt-suffix \"babble\" --bg-snrs \"20:17:15:13\" --num-bg-noises \"3:4:5:6:7\" \\\n",
    "    --bg-noise-dir \"data/musan_speech\" data/${clean_train_set}_tp data/${clean_train_set}_tp_babble\n",
    "  \n",
    "  # Combine reverb, noise, music, and babble into one directory.\n",
    "  utils/combine_data.sh data/${train_set} data/${clean_train_set}_tp_noise data/${clean_train_set}_tp_music \\\n",
    "    data/${clean_train_set}_tp_babble\n",
    "  echo \"Finished combining everything\"\n",
    "  echo \"\"\n",
    "  \n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "# This script is called from local/nnet3/run_tdnn.sh and local/chain/run_tdnn.sh\n",
    "# (and may eventually be called by more scripts). It contains the common feature\n",
    "# preparation and ivector-related parts of the script. See those scripts for \n",
    "# examples of usage.\n",
    "\n",
    "stage=0\n",
    "clean_train_set=train_nodev\n",
    "train_set=${clean_train_set}_tp_aug\n",
    "dev_set=\n",
    "clean_test_sets=\"eval1 eval2 eval3\"\n",
    "test_sets=\"noisy_eval1 noisy_eval2 noisy_eval3\"\n",
    "gmm=tri4\n",
    "\n",
    "nnet3_affix=\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "if [ -e data/train_dev ] ;then\n",
    "  dev_set=train_dev\n",
    "fi\n",
    "\n",
    "gmm_dir=exp/${gmm}\n",
    "ali_dir=exp/${gmm}_ali_${clean_train_set}_tp\n",
    "\n",
    "for f in data/${clean_train_set}/feats.scp ${gmm_dir}/final.mdl; do\n",
    "  if [ ! -f $f ]; then\n",
    "    echo \"$0: expected file $f to exist\"\n",
    "    exit 1\n",
    "  fi\n",
    "done\n",
    "\n",
    "if [ $stage -le 1 ]; then\n",
    "  echo \"$0: making MFCC featuresfor low-resolution speed-then-reverb-perturbed data\"\n",
    "  steps/make_mfcc.sh --cmd \"$train_cmd\" --nj 24 data/${clean_train_set}_tp_reverb || exit 1;\n",
    "  steps/compute_cmvn_stats.sh data/${clean_train_set}_tp_reverb || exit 1;\n",
    "  utils/fix_data_dir.sh data/${clean_train_set}_tp_reverb\n",
    "\n",
    "  # Combine the clean and augmented subset.  This is now roughly\n",
    "  # quaruple the size of the original clean list.\n",
    "  utils/combine_data.sh data/train_robust data/${clean_train_set}_tp data/${train_set} data/${clean_train_set}_tp_reverb\n",
    "  calcdur data/train_robust/segments\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "# This script is called from local/nnet3/run_tdnn.sh and local/chain/run_tdnn.sh\n",
    "# (and may eventually be called by more scripts). It contains the common feature\n",
    "# preparation and ivector-related parts of the script. See those scripts for \n",
    "# examples of usage.\n",
    "\n",
    "stage=0\n",
    "clean_train_set=train_nodev\n",
    "train_set=${clean_train_set}_tp_aug\n",
    "dev_set=\n",
    "clean_test_sets=\"eval1 eval2 eval3\"\n",
    "test_sets=\"noisy_eval1 noisy_eval2 noisy_eval3\"\n",
    "gmm=tri4\n",
    "\n",
    "nnet3_affix=\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "if [ -e data/train_dev ] ;then\n",
    "  dev_set=train_dev\n",
    "fi\n",
    "\n",
    "gmm_dir=exp/${gmm}\n",
    "ali_dir=exp/${gmm}_ali_${clean_train_set}_tp\n",
    "\n",
    "for f in data/${clean_train_set}/feats.scp ${gmm_dir}/final.mdl; do\n",
    "  if [ ! -f $f ]; then\n",
    "    echo \"$0: expected file $f to exist\"\n",
    "    exit 1\n",
    "  fi\n",
    "done\n",
    "\n",
    "export LC_ALL=C;\n",
    "if [ $stage -le 1 ]; then\n",
    "  #rm -fr data/train_robust\n",
    "\n",
    "  # fix speakers for the augmentation case, turn noisy speech into their own \"speakers\"\n",
    "  #cat data/${train_set}/utt2spk_BACKUP | sed 's/.*\\(babble\\|music\\|noise\\).*/&-\\1/g' | sort --parallel=8 \\\n",
    "  #  > data/${train_set}/utt2spk\n",
    "  #sort -k 2 data/${train_set}/utt2spk | utils/utt2spk_to_spk2utt.pl > data/${train_set}/spk2utt || exit 1;\n",
    "  # because of this, we have to recompute the cmvn stats\n",
    "  #steps/compute_cmvn_stats.sh data/${train_set} || exit 1;\n",
    "  #utils/fix_data_dir.sh data/${train_set}\n",
    "  \n",
    "  # Combine the clean and augmented subset.  This is now roughly\n",
    "  # quaruple the size of the original clean list.\n",
    "  utils/combine_data.sh data/train_robust data/${clean_train_set}_tp_clean \\\n",
    "                        data/${train_set} data/${clean_train_set}_tp_reverb\n",
    "  utils/fix_data_dir.sh data/train_robust\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_utts_total=$(wc -l <data/${train_set}_hires/utt2spk)\n",
    "  num_utts=$[$num_utts_total/3]\n",
    "  utils/data/subset_data_dir.sh data/${train_set}_hires \\\n",
    "    $num_utts ${temp_data_root}/${train_set}_hires_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "clean_train_set=train_nodev\n",
    "tempo_aug_set=${clean_train_set}_tp\n",
    "\n",
    "for settype in data/${tempo_aug_set}{,_rvb,_noise,_music,_babble}; do\n",
    "  grep -v 'sp0.8-' ${settype}/text | cut -d' ' -f1 > ${settype}/uttlist_noslow\n",
    "  utils/data/subset_data_dir.sh --utt-list ${settype}/uttlist_noslow ${settype} ${settype}_noslow\n",
    "done\n",
    "\n",
    "num_utts_per_set=$(wc -l < data/${tempo_aug_set}_noslow/utt2spk)\n",
    "num_utts_wanted=$[$num_utts_per_set * 5 / 2]\n",
    "\n",
    "# special treatment for clean and random noise\n",
    "utils/data/subset_data_dir.sh data/${tempo_aug_set}_noslow $[$num_utts_wanted * 3 / 16] \\\n",
    "  data/${tempo_aug_set}_noslow_div\n",
    "utils/data/subset_data_dir.sh data/${tempo_aug_set}_noise_noslow $[$num_utts_wanted / 16] \\\n",
    "  data/${tempo_aug_set}_noise_noslow_div\n",
    "for cortype in rvb music babble; do\n",
    "  utils/data/subset_data_dir.sh data/${tempo_aug_set}_${cortype}_noslow $[$num_utts_wanted / 4] \\\n",
    "    data/${tempo_aug_set}_${cortype}_noslow_div\n",
    "done\n",
    "\n",
    "utils/combine_data.sh data/train_robust data/${tempo_aug_set}_noslow data/${tempo_aug_set}_rvb_noslow \\\n",
    "  data/${tempo_aug_set}_noise_noslow data/${tempo_aug_set}_music_noslow data/${tempo_aug_set}_babble_noslow\n",
    "\n",
    "\n",
    "utils/combine_data.sh data/train_robust_small data/${tempo_aug_set}_noslow_div \\\n",
    "  data/${tempo_aug_set}_rvb_noslow_div data/${tempo_aug_set}_noise_noslow_div \\\n",
    "  data/${tempo_aug_set}_music_noslow_div data/${tempo_aug_set}_babble_noslow_div\n",
    "\n",
    "\n",
    "rm -r data/*noslow*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "# fixed babble, not redoing ivector extractor training\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "clean_train_set=train_nodev\n",
    "tempo_aug_set=${clean_train_set}_tp\n",
    "\n",
    "for settype in data/${tempo_aug_set}{,_rvb,_noise,_music,_babble}; do\n",
    "  grep -v 'sp0.8-' ${settype}/text | cut -d' ' -f1 > ${settype}/uttlist_noslow\n",
    "  utils/data/subset_data_dir.sh --utt-list ${settype}/uttlist_noslow ${settype} ${settype}_noslow\n",
    "done\n",
    "\n",
    "utils/combine_data.sh data/train_robust data/${tempo_aug_set}_noslow data/${tempo_aug_set}_rvb_noslow \\\n",
    "  data/${tempo_aug_set}_noise_noslow data/${tempo_aug_set}_music_noslow data/${tempo_aug_set}_babble_noslow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "# cheat set creation\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "clean_train_set=train_nodev\n",
    "tempo_aug_set=${clean_train_set}_tp\n",
    "fake_name=fake_tp\n",
    "cheat_set=cheat_set_for_lats\n",
    "\n",
    "# there's a discrepancy between rvb's prefix \"reverb1-\" so we're isolating\n",
    "utils/copy_data_dir.sh --spk-prefix \"reverb1-\" --utt-prefix \"reverb1-\" \\\n",
    "  data/${tempo_aug_set}_noslow data/${fake_name}_rvb_noslow\n",
    "# full fakery\n",
    "awk '{print \"reverb1-\"$0}' data/${fake_name}_rvb_noslow/reco2dur > data/${fake_name}_rvb_noslow/reco2dur_fake\n",
    "mv data/${fake_name}_rvb_noslow/reco2dur_fake data/${fake_name}_rvb_noslow/reco2dur\n",
    "awk '{print $1\" reverb1-\"$2\" \"$3\" \"$4}' data/${fake_name}_rvb_noslow/segments \\\n",
    "  > data/${fake_name}_rvb_noslow/segments_fake\n",
    "mv data/${fake_name}_rvb_noslow/segments_fake data/${fake_name}_rvb_noslow/segments\n",
    "awk '{print \"reverb1-\"$0}' data/${fake_name}_rvb_noslow/wav.scp > data/${fake_name}_rvb_noslow/wav.scp_fake\n",
    "mv data/${fake_name}_rvb_noslow/wav.scp_fake data/${fake_name}_rvb_noslow/wav.scp\n",
    "\n",
    "# musan\n",
    "for settype in {noise,music,babble}; do\n",
    "  utils/copy_data_dir.sh --spk-prefix ${settype}- --utt-prefix ${settype}- \\\n",
    "    data/${tempo_aug_set}_noslow data/${fake_name}_${settype}_noslow\n",
    "  for orig in {reco2dur,segments,wav.scp}; do\n",
    "    sed \"s/reverb1/$settype/g\" data/${fake_name}_rvb_noslow/$orig \\\n",
    "      > data/${fake_name}_${settype}_noslow/${orig}_fake\n",
    "    mv data/${fake_name}_${settype}_noslow/${orig}_fake data/${fake_name}_${settype}_noslow/$orig\n",
    "  done\n",
    "done\n",
    "\n",
    "utils/combine_data.sh data/${cheat_set} data/${tempo_aug_set}_noslow data/${fake_name}_rvb_noslow \\\n",
    "  data/${fake_name}_noise_noslow data/${fake_name}_music_noslow data/${fake_name}_babble_noslow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iVector common starts here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "stage=0\n",
    "train_set=train_robust\n",
    "gmm=tri4\n",
    "cheat_set=cheat_set_for_lats\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "gmm_dir=exp/${gmm}\n",
    "ali_dir=exp/${gmm}_ali_${train_set}\n",
    "\n",
    "if [ $stage -le 2 ]; then\n",
    "  echo \"$0: aligning with the perturbed low-resolution data\"\n",
    "  steps/align_fmllr.sh --nj 32 --cmd \"$train_cmd\" \\\n",
    "    data/${cheat_set} data/lang $gmm_dir $ali_dir || exit 1;\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "stage=0\n",
    "train_set=train_robust\n",
    "gmm=tri4\n",
    "cheat_set=cheat_set_for_lats\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "gmm_dir=exp/${gmm}\n",
    "ali_dir=exp/${gmm}_ali_${train_set}\n",
    "\n",
    "if [ $stage -le 3 ]; then\n",
    "  steps/train_sat.sh  --cmd \"$train_cmd\" \\\n",
    "    13000 300000 data/${cheat_set} data/lang $ali_dir exp/tri5\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "stage=0\n",
    "train_set=train_robust\n",
    "dev_set=\n",
    "test_sets=\n",
    "gmm=tri5\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "if [ $stage -le 3 ]; then\n",
    "  #Create high-resolution MFCC features (with 40 cepstra instead of 13).\n",
    "  # this shows how you can split across multiple file-systems.\n",
    "  echo \"$0: creating high-resolution MFCC features\"\n",
    "  mfccdir=data/${train_set}_hires/data\n",
    "\n",
    "  for datadir in ${train_set}; do\n",
    "    utils/copy_data_dir.sh data/$datadir data/${datadir}_hires\n",
    "  done\n",
    "\n",
    "  # do volume-perturbation on the training data prior to extracting hires\n",
    "  # features; this helps make trained nnets more invariant to test data volume\n",
    "  utils/data/perturb_data_dir_volume.sh data/${train_set}_hires || exit 1;\n",
    "\n",
    "  # generate high-resolution MFCC feautres\n",
    "  for datadir in ${train_set}; do\n",
    "    steps/make_mfcc.sh --nj 32 --mfcc-config conf/mfcc_hires.conf \\\n",
    "      --cmd \"$train_cmd\" data/${datadir}_hires || exit 1;\n",
    "    steps/compute_cmvn_stats.sh data/${datadir}_hires || exit 1;\n",
    "    utils/fix_data_dir.sh data/${datadir}_hires || exit 1;\n",
    "  done\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "stage=0\n",
    "train_set=train_robust\n",
    "dev_set=\n",
    "test_sets=\"eval1 eval2 eval3\"\n",
    "gmm=tri5\n",
    "\n",
    "nnet3_affix=\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "if [ $stage -le 4 ]; then\n",
    "  echo \"$0: train the diagonal UBM.\"\n",
    "  # There's no data, so use all\n",
    "  mkdir -p exp/nnet3${nnet3_affix}/diag_ubm\n",
    "  #temp_data_root=exp/nnet3${nnet3_affix}/diag_ubm\n",
    "\n",
    "  #num_utts_total=$(wc -l <data/${train_set}_hires/utt2spk)\n",
    "  #num_utts=$[$num_utts_total]\n",
    "  #utils/data/subset_data_dir.sh data/${train_set}_hires \\\n",
    "  #  $num_utts ${temp_data_root}/${train_set}_hires_subset\n",
    "\n",
    "  echo \"$0: computing a PCA transform from the hires data.\"\n",
    "  steps/online/nnet2/get_pca_transform.sh --cmd \"$train_cmd\" \\\n",
    "    --splice-opts \"--left-context=4 --right-context=4\" \\\n",
    "    --max-utts 30000 --subsample 2 \\\n",
    "    #${temp_data_root}/${train_set}_hires_subset \\\n",
    "    data/${train_set}_small_hires \\\n",
    "    exp/nnet3${nnet3_affix}/pca_transform\n",
    "\n",
    "  echo \"$0: training the diagonal UBM.\"\n",
    "  # Use 512 Gaussians in the UBM.\n",
    "  steps/online/nnet2/train_diag_ubm.sh --cmd \"$train_cmd\" --nj 32 \\\n",
    "    --num-frames 500000 \\\n",
    "    #--num-threads 8 ${temp_data_root}/${train_set}_hires_subset 512 \\\n",
    "    data/${train_set}_small_hires 512 \\\n",
    "    exp/nnet3${nnet3_affix}/pca_transform exp/nnet3${nnet3_affix}/diag_ubm\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "stage=0\n",
    "train_set=train_robust\n",
    "dev_set=\n",
    "test_sets=\n",
    "gmm=tri5\n",
    "\n",
    "nnet3_affix=\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "if [ $stage -le 5 ]; then\n",
    "  # Train the iVector extractor.Use all of the speed-perturbed data since iVector extractors\n",
    "  # can be sensitive to the amount of data. The script defaults to an iVector dimension of 100\n",
    "  # even though $nj is just 2 (10 on high end set ups), each job uses multiple processes and threads.\n",
    "  echo \"$0: training the iVector extractor\"\n",
    "  steps/online/nnet2/train_ivector_extractor.sh --cmd \"$train_cmd\" --nj 4 \\\n",
    "    #exp/nnet3${nnet3_affix}/diag_ubm/${train_set}_hires_subset\n",
    "    data/${train_set}_small_hires exp/nnet3${nnet3_affix}/diag_ubm \\\n",
    "    exp/nnet3${nnet3_affix}/extractor || exit 1;\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "stage=0\n",
    "train_set=train_robust\n",
    "dev_set=\n",
    "test_sets=\"eval1 eval2 eval3\"\n",
    "gmm=tri5\n",
    "\n",
    "nnet3_affix=\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "if [ $stage -le 6 ]; then\n",
    "  # We extract iVectors on the speed-perturbed training data after combining\n",
    "  # short segments, which will be what we train the system on. With --utts-per-spk-max 2,\n",
    "  # the scripts pairs the utterances into twos, and treats each of these pairs\n",
    "  # as one speaker; this gives more diversity in iVectors.\n",
    "  # Note that these are extracted 'online'.\n",
    "\n",
    "  # Note, we don't encode the 'max2' in the name of the iVectordir even though\n",
    "  # that's the data we extract the iVectors from, as it's still going to be \n",
    "  # valid for the non-'max2' data, the utterance list is the same.\n",
    "  ivectordir=exp/nnet3${nnet3_affix}/ivectors_${train_set}_hires\n",
    "\n",
    "  # having a larger number of speakers is helpful for generalization, and to\n",
    "  # handle per-utterance decoding well (iVector starts at zero).\n",
    "  temp_data_root=${ivectordir}\n",
    "  utils/data/modify_speaker_info.sh --utts-per-spk-max 2 \\\n",
    "    data/${train_set}_hires ${temp_data_root}/${train_set}_hires_max2\n",
    "\n",
    "  steps/online/nnet2/extract_ivectors_online.sh --cmd \"$train_cmd\" --nj 24 \\\n",
    "    ${temp_data_root}/${train_set}_hires_max2 \\\n",
    "    exp/nnet3${nnet3_affix}/extractor $ivectordir\n",
    "\n",
    "  # Also extract iVectors for the test data, but in this case we don't need the speed\n",
    "  # perturbation (sp).\n",
    "  #for datadir in $dev_set $test_sets; do\n",
    "  #  steps/online/nnet2/extract_ivectors_online.sh --cmd \"$train_cmd\" --nj 10 \\\n",
    "  #    data/${datadir}_hires exp/nnet3${nnet3_affix}/extractor \\\n",
    "  #    exp/nnet3${nnet3_affix}/ivectors_${datadir}_hires\n",
    "  #done\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural net training starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "stage=0\n",
    "train_set=train_robust\n",
    "cheat_set=cheat_set_for_lats\n",
    "gmm=tri5\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "gmm_dir=exp/${gmm}\n",
    "ali_dir=exp/${gmm}_ali_${train_set}\n",
    "\n",
    "if [ $stage -le 2 ]; then\n",
    "  echo \"$0: aligning with the perturbed low-resolution data\"\n",
    "  steps/align_fmllr.sh --nj 32 --cmd \"$train_cmd\" \\\n",
    "    data/${cheat_set} data/lang $gmm_dir $ali_dir || exit 1;\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash \n",
      "lrwxrwxrwx 1 nobody nogroup 6 Jan 11 17:27 exp/tri5/final.mdl -> 35.mdl\n",
      "-rw-r--r-- 1 nobody nogroup 1.2G Jan 11 08:15 data/train_robust_hires/feats.scp\n",
      "-rw-r--r-- 1 nobody nogroup 1.1G Jan 11 13:51 exp/nnet3/ivectors_train_robust_hires/ivector_online.scp\n",
      "-rw-r--r-- 1 nobody nogroup 1.2G Jan 11 02:58 data/train_robust/feats.scp\n",
      "-rw-r--r-- 1 nobody nogroup 19M Jan 12 04:12 exp/tri5_ali_train_robust/ali.1.gz\n",
      "CPU times: user 0 ns, sys: 16 ms, total: 16 ms\n",
      "Wall time: 353 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "# First the options that are passed through to run_ivector_common.sh\n",
    "# (some of which are also used in this script directly).\n",
    "gmm=tri5\n",
    "train_set=train_robust\n",
    "nnet3_affix=\n",
    "# The rest are configs specific to this script. Most of the parameters\n",
    "# are just hardcoded at this level, in the commands below.\n",
    "affix=7n  # affix for the TDNN directory name\n",
    "tree_affix=\n",
    "\n",
    "# End configuration section.\n",
    "echo \"$0 $@\"  # Print the command line for logging\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "if ! cuda-compiled; then\n",
    "  cat <<EOF && exit 1\n",
    "This script is intended to be used with GPUs but you have not compiled Kaldi with CUDA\n",
    "If you want to use GPUs (and have them), go to src/, and configure and make on a machine\n",
    "where \"nvcc\" is installed.\n",
    "EOF\n",
    "fi\n",
    "\n",
    "\n",
    "gmm_dir=exp/$gmm\n",
    "ali_dir=exp/${gmm}_ali_${train_set}\n",
    "tree_dir=exp/chain${nnet3_affix}/tree${tree_affix:+_$tree_affix}\n",
    "lang=data/lang_chain\n",
    "lat_dir=exp/chain${nnet3_affix}/${gmm}_${train_set}_lats\n",
    "dir=exp/chain${nnet3_affix}/tdnn${affix}\n",
    "train_data_dir=data/${train_set}_hires\n",
    "lores_train_data_dir=data/${train_set}\n",
    "train_ivector_dir=exp/nnet3${nnet3_affix}/ivectors_${train_set}_hires\n",
    "\n",
    "for fa in $gmm_dir/final.mdl $train_data_dir/feats.scp $train_ivector_dir/ivector_online.scp \\\n",
    "  $lores_train_data_dir/feats.scp $ali_dir/ali.1.gz; do\n",
    "  # unfortunately, this check won't work on jupyter's bash magic\n",
    "  #[ ! -f $fa ] && echo \"$0; expected file $fa to exist\" && exit 1;\n",
    "  ls -ltrh $fa\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "# First the options that are passed through to run_ivector_common.sh\n",
    "# (some of which are also used in this script directly).\n",
    "stage=0\n",
    "gmm=tri5\n",
    "train_set=train_robust\n",
    "cheat_set=cheat_set_for_lats\n",
    "nnet3_affix=\n",
    "# The rest are configs specific to this script. Most of the parameters\n",
    "# are just hardcoded at this level, in the commands below.\n",
    "affix=7n  # affix for the TDNN directory name\n",
    "tree_affix=\n",
    "\n",
    "# End configuration section.\n",
    "echo \"$0 $@\"  # Print the command line for logging\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "if ! cuda-compiled; then\n",
    "  cat <<EOF && exit 1\n",
    "This script is intended to be used with GPUs but you have not compiled Kaldi with CUDA\n",
    "If you want to use GPUs (and have them), go to src/, and configure and make on a machine\n",
    "where \"nvcc\" is installed.\n",
    "EOF\n",
    "fi\n",
    "\n",
    "\n",
    "gmm_dir=exp/$gmm\n",
    "ali_dir=exp/${gmm}_ali_${train_set}\n",
    "tree_dir=exp/chain${nnet3_affix}/tree${tree_affix:+_$tree_affix}\n",
    "lang=data/lang_chain\n",
    "lat_dir=exp/chain${nnet3_affix}/${gmm}_${train_set}_lats\n",
    "dir=exp/chain${nnet3_affix}/tdnn${affix}\n",
    "train_data_dir=data/${train_set}_hires\n",
    "#lores_train_data_dir=data/${train_set}\n",
    "lores_train_data_dir=data/${cheat_set}\n",
    "train_ivector_dir=exp/nnet3${nnet3_affix}/ivectors_${train_set}_hires\n",
    "\n",
    "if [ $stage -le 9 ]; then\n",
    "  # Get the alignments as lattices (gives the LF-MMI training more freedom).\n",
    "  # use the same num-jobs as the alignments\n",
    "  steps/align_fmllr_lats.sh --nj 36 --cmd \"$train_cmd\" ${lores_train_data_dir} \\\n",
    "    data/lang $gmm_dir $lat_dir\n",
    "  rm $lat_dir/fsts.*.gz # save space\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "# First the options that are passed through to run_ivector_common.sh\n",
    "# (some of which are also used in this script directly).\n",
    "stage=0\n",
    "\n",
    "# End configuration section.\n",
    "echo \"$0 $@\"  # Print the command line for logging\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "if ! cuda-compiled; then\n",
    "  cat <<EOF && exit 1\n",
    "This script is intended to be used with GPUs but you have not compiled Kaldi with CUDA\n",
    "If you want to use GPUs (and have them), go to src/, and configure and make on a machine\n",
    "where \"nvcc\" is installed.\n",
    "EOF\n",
    "fi\n",
    "\n",
    "lang=data/lang_chain\n",
    "\n",
    "if [ $stage -le 10 ]; then\n",
    "  # Create a version of the lang/ directory that has one state per phone in the\n",
    "  # topo file. [note, it really has two states.. the first one is only repeated\n",
    "  # once, the second one has zero or more repeats.]\n",
    "  rm -rf $lang\n",
    "  cp -r data/lang $lang\n",
    "  silphonelist=$(cat $lang/phones/silence.csl) || exit 1;\n",
    "  nonsilphonelist=$(cat $lang/phones/nonsilence.csl) || exit 1;\n",
    "  # Use our special topology... note that later on may have to tune this\n",
    "  # topology.\n",
    "  steps/nnet3/chain/gen_topo.py $nonsilphonelist $silphonelist >$lang/topo\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "# First the options that are passed through to run_ivector_common.sh\n",
    "# (some of which are also used in this script directly).\n",
    "stage=0\n",
    "decode_nj=10\n",
    "train_set=train_robust\n",
    "cheat_set=cheat_set_for_lats\n",
    "dev_set=\n",
    "test_sets=\"eval1 eval2 eval3\"\n",
    "gmm=tri5\n",
    "nnet3_affix=\n",
    "\n",
    "# The rest are configs specific to this script. Most of the parameters\n",
    "# are just hardcoded at this level, in the commands below.\n",
    "affix=7n  # affix for the TDNN directory name\n",
    "tree_affix=\n",
    "train_stage=-10\n",
    "get_egs_stage=-10\n",
    "decode_iter=\n",
    "\n",
    "# training options\n",
    "# training chunk-options\n",
    "decode_iter=\n",
    "num_epochs=10\n",
    "initial_effective_lrate=0.001\n",
    "final_effective_lrate=0.0001\n",
    "leftmost_questions_truncate=-1\n",
    "max_param_change=2.0\n",
    "final_layer_normalize_target=0.5\n",
    "num_jobs_initial=1\n",
    "num_jobs_final=3\n",
    "minibatch_size=128,64\n",
    "#frames_per_eg=150,140,100\n",
    "frames_per_eg=75,70,50\n",
    "remove_egs=false\n",
    "common_egs_dir=\n",
    "xent_regularize=0.1\n",
    "\n",
    "test_online_decoding=false  # if true, it will run the last decoding stage.\n",
    "\n",
    "# End configuration section.\n",
    "echo \"$0 $@\"  # Print the command line for logging\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "gmm_dir=exp/$gmm\n",
    "ali_dir=exp/${gmm}_ali_${train_set}\n",
    "tree_dir=exp/chain${nnet3_affix}/tree${tree_affix:+_$tree_affix}\n",
    "lang=data/lang_chain\n",
    "lat_dir=exp/chain${nnet3_affix}/${gmm}_${train_set}_lats\n",
    "dir=exp/chain${nnet3_affix}/tdnn${affix}\n",
    "train_data_dir=data/${train_set}_hires\n",
    "#lores_train_data_dir=data/${train_set}\n",
    "lores_train_data_dir=data/${cheat_set}\n",
    "train_ivector_dir=exp/nnet3${nnet3_affix}/ivectors_${train_set}_hires\n",
    "\n",
    "if [ $stage -le 11 ]; then\n",
    "  # Build a tree using our new topology. This is the critically different\n",
    "  # step compared with other recipes.\n",
    "  steps/nnet3/chain/build_tree.sh --frame-subsampling-factor 3 \\\n",
    "      --leftmost-questions-truncate $leftmost_questions_truncate \\\n",
    "      --context-opts \"--context-width=2 --central-position=1\" \\\n",
    "      --cmd \"$train_cmd\" 11000 ${lores_train_data_dir} $lang $ali_dir $tree_dir\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "# First the options that are passed through to run_ivector_common.sh\n",
    "# (some of which are also used in this script directly).\n",
    "stage=0\n",
    "decode_nj=10\n",
    "train_set=train_robust\n",
    "dev_set=\n",
    "test_sets=\n",
    "gmm=tri5\n",
    "nnet3_affix=\n",
    "\n",
    "# The rest are configs specific to this script. Most of the parameters\n",
    "# are just hardcoded at this level, in the commands below.\n",
    "affix=7n_robust  # affix for the TDNN directory name\n",
    "tree_affix=\n",
    "train_stage=-10\n",
    "get_egs_stage=-10\n",
    "decode_iter=\n",
    "\n",
    "# training options\n",
    "# training chunk-options\n",
    "decode_iter=\n",
    "num_epochs=10\n",
    "initial_effective_lrate=0.001\n",
    "final_effective_lrate=0.0001\n",
    "leftmost_questions_truncate=-1\n",
    "max_param_change=2.0\n",
    "final_layer_normalize_target=0.5\n",
    "num_jobs_initial=1\n",
    "num_jobs_final=3\n",
    "#minibatch_size=128,64\n",
    "minibatch_size=256,128\n",
    "#frames_per_eg=150,140,100\n",
    "frames_per_eg=75,70,50\n",
    "remove_egs=false\n",
    "common_egs_dir=\n",
    "xent_regularize=0.1\n",
    "\n",
    "test_online_decoding=false  # if true, it will run the last decoding stage.\n",
    "\n",
    "# End configuration section.\n",
    "echo \"$0 $@\"  # Print the command line for logging\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "gmm_dir=exp/$gmm\n",
    "ali_dir=exp/${gmm}_ali_${train_set}\n",
    "tree_dir=exp/chain${nnet3_affix}/tree${tree_affix:+_$tree_affix}\n",
    "lang=data/lang_chain\n",
    "lat_dir=exp/chain${nnet3_affix}/${gmm}_${train_set}_lats\n",
    "dir=exp/chain${nnet3_affix}/tdnn${affix}\n",
    "train_data_dir=data/${train_set}_hires\n",
    "lores_train_data_dir=data/${train_set}\n",
    "train_ivector_dir=exp/nnet3${nnet3_affix}/ivectors_${train_set}_hires\n",
    "\n",
    "if [ $stage -le 12 ]; then\n",
    "  echo \"$0: creating neural net configs using the xconfig parser\";\n",
    "\n",
    "  num_targets=$(tree-info $tree_dir/tree |grep num-pdfs|awk '{print $2}')\n",
    "  learning_rate_factor=$(echo \"print 0.5/$xent_regularize\" | python2)\n",
    "  opts=\"l2-regularize=0.002\"\n",
    "  linear_opts=\"orthonormal-constraint=1.0\"\n",
    "  output_opts=\"l2-regularize=0.0005 bottleneck-dim=256\"\n",
    "\n",
    "  mkdir -p $dir/configs\n",
    "\n",
    "  cat <<EOF > $dir/configs/network.xconfig\n",
    "  input dim=100 name=ivector\n",
    "  input dim=40 name=input\n",
    "\n",
    "  # please note that it is important to have input layer with the name=input\n",
    "  # as the layer immediately preceding the fixed-affine-layer to enable\n",
    "  # the use of short notation for the descriptor\n",
    "  fixed-affine-layer name=lda input=Append(-1,0,1,ReplaceIndex(ivector, t, 0)) affine-transform-file=$dir/configs/lda.mat\n",
    "\n",
    "  # the first splicing is moved before the lda layer, so no splicing here\n",
    "  relu-batchnorm-layer name=tdnn1 $opts dim=1280\n",
    "  linear-component name=tdnn2l dim=256 $linear_opts input=Append(-1,0)\n",
    "  relu-batchnorm-layer name=tdnn2 $opts input=Append(0,1) dim=1280\n",
    "  linear-component name=tdnn3l dim=256 $linear_opts\n",
    "  relu-batchnorm-layer name=tdnn3 $opts dim=1280\n",
    "  linear-component name=tdnn4l dim=256 $linear_opts input=Append(-1,0)\n",
    "  relu-batchnorm-layer name=tdnn4 $opts input=Append(0,1) dim=1280\n",
    "  linear-component name=tdnn5l dim=256 $linear_opts\n",
    "  relu-batchnorm-layer name=tdnn5 $opts dim=1280 input=Append(tdnn5l, tdnn3l)\n",
    "  linear-component name=tdnn6l dim=256 $linear_opts input=Append(-3,0)\n",
    "  relu-batchnorm-layer name=tdnn6 $opts input=Append(0,3) dim=1280\n",
    "  linear-component name=tdnn7l dim=256 $linear_opts input=Append(-3,0)\n",
    "  relu-batchnorm-layer name=tdnn7 $opts input=Append(0,3,tdnn6l,tdnn4l,tdnn2l) dim=1280\n",
    "  linear-component name=tdnn8l dim=256 $linear_opts input=Append(-3,0)\n",
    "  relu-batchnorm-layer name=tdnn8 $opts input=Append(0,3) dim=1280\n",
    "  linear-component name=tdnn9l dim=256 $linear_opts input=Append(-3,0)\n",
    "  relu-batchnorm-layer name=tdnn9 $opts input=Append(0,3,tdnn8l,tdnn6l,tdnn4l) dim=1280\n",
    "  linear-component name=tdnn10l dim=256 $linear_opts input=Append(-3,0)\n",
    "  relu-batchnorm-layer name=tdnn10 $opts input=Append(0,3) dim=1280\n",
    "  linear-component name=tdnn11l dim=256 $linear_opts input=Append(-3,0)\n",
    "  relu-batchnorm-layer name=tdnn11 $opts input=Append(0,3,tdnn10l,tdnn8l,tdnn6l) dim=1280\n",
    "  linear-component name=prefinal-l dim=256 $linear_opts\n",
    "\n",
    "  relu-batchnorm-layer name=prefinal-chain input=prefinal-l $opts dim=1280\n",
    "  output-layer name=output include-log-softmax=false dim=$num_targets $output_opts\n",
    "\n",
    "  relu-batchnorm-layer name=prefinal-xent input=prefinal-l $opts dim=1280\n",
    "  output-layer name=output-xent dim=$num_targets learning-rate-factor=$learning_rate_factor $output_opts\n",
    "EOF\n",
    "  steps/nnet3/xconfig_to_configs.py --xconfig-file $dir/configs/network.xconfig --config-dir $dir/configs/\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "# First the options that are passed through to run_ivector_common.sh\n",
    "# (some of which are also used in this script directly).\n",
    "stage=0\n",
    "decode_nj=10\n",
    "train_set=train_robust\n",
    "dev_set=\n",
    "test_sets=\n",
    "gmm=tri5\n",
    "nnet3_affix=\n",
    "\n",
    "# The rest are configs specific to this script. Most of the parameters\n",
    "# are just hardcoded at this level, in the commands below.\n",
    "affix=7n  # affix for the TDNN directory name\n",
    "tree_affix=\n",
    "train_stage=-10\n",
    "get_egs_stage=-10\n",
    "decode_iter=\n",
    "\n",
    "# training options\n",
    "# training chunk-options\n",
    "decode_iter=\n",
    "num_epochs=24\n",
    "initial_effective_lrate=0.001\n",
    "final_effective_lrate=0.0001\n",
    "leftmost_questions_truncate=-1\n",
    "max_param_change=2.0\n",
    "final_layer_normalize_target=0.5\n",
    "num_jobs_initial=1\n",
    "num_jobs_final=1\n",
    "#minibatch_size=128,64\n",
    "minibatch_size=256,128\n",
    "#frames_per_eg=150,140,100\n",
    "frames_per_eg=75,70,50\n",
    "remove_egs=false\n",
    "common_egs_dir=\n",
    "xent_regularize=0.1\n",
    "\n",
    "test_online_decoding=false  # if true, it will run the last decoding stage.\n",
    "\n",
    "# End configuration section.\n",
    "echo \"$0 $@\"  # Print the command line for logging\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "gmm_dir=exp/$gmm\n",
    "ali_dir=exp/${gmm}_ali_${train_set}\n",
    "tree_dir=exp/chain${nnet3_affix}/tree${tree_affix:+_$tree_affix}\n",
    "lang=data/lang_chain\n",
    "lat_dir=exp/chain${nnet3_affix}/${gmm}_${train_set}_lats\n",
    "dir=exp/chain${nnet3_affix}/tdnn${affix}\n",
    "train_data_dir=data/${train_set}_hires\n",
    "lores_train_data_dir=data/${train_set}\n",
    "train_ivector_dir=exp/nnet3${nnet3_affix}/ivectors_${train_set}_hires\n",
    "\n",
    "if [ $stage -le 13 ]; then\n",
    "  steps/nnet3/chain/train.py --stage $train_stage \\\n",
    "    --cmd \"$train_cmd\" \\\n",
    "    --feat.online-ivector-dir $train_ivector_dir \\\n",
    "    --feat.cmvn-opts \"--norm-means=false --norm-vars=false\" \\\n",
    "    --chain.xent-regularize $xent_regularize \\\n",
    "    --chain.leaky-hmm-coefficient 0.1 \\\n",
    "    --chain.l2-regularize 0.0 \\\n",
    "    --chain.apply-deriv-weights false \\\n",
    "    --chain.lm-opts=\"--num-extra-lm-states=2000\" \\\n",
    "    --egs.dir \"$common_egs_dir\" \\\n",
    "    --egs.stage $get_egs_stage \\\n",
    "    --egs.opts \"--frames-overlap-per-eg 0\" \\\n",
    "    --egs.chunk-width $frames_per_eg \\\n",
    "    --trainer.num-chunk-per-minibatch 128 \\\n",
    "    --trainer.frames-per-iter 1500000 \\\n",
    "    --trainer.num-epochs $num_epochs \\\n",
    "    --trainer.optimization.num-jobs-initial $num_jobs_initial \\\n",
    "    --trainer.optimization.num-jobs-final $num_jobs_final \\\n",
    "    --trainer.optimization.initial-effective-lrate 0.001 \\\n",
    "    --trainer.optimization.final-effective-lrate 0.0001 \\\n",
    "    --trainer.max-param-change 2.0 \\\n",
    "    --use-gpu=wait \\\n",
    "    --cleanup.remove-egs $remove_egs \\\n",
    "    --feat-dir $train_data_dir \\\n",
    "    --tree-dir $tree_dir \\\n",
    "    --lat-dir $lat_dir \\\n",
    "    --dir $dir  || exit 1;\n",
    "\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoding graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "lmDir=data/exp/lm\n",
    "srcdict=data/exp/dict_bigtext/lexicon.txt\n",
    "dir=data/exp/dict_bigtext\n",
    "\n",
    "export LC_ALL=C\n",
    "\n",
    "cat data/exp/dict_bigtext/ALL_dict | sed '/<sp>/d; /<unk>/d; /^[ ]*$/d' | sort --parallel=8 | uniq > $srcdict\n",
    "\n",
    "\n",
    "cat $srcdict > $dir/lexicon1.txt || exit 1;\n",
    "\n",
    "#cat $dir/lexicon1.txt | awk '{ for(n=2;n<=NF;n++){ phones[$n] = 1; }} END{for (p in phones) print p;}' | \\\n",
    "#  grep -v SIL > $dir/nonsilence_phones.txt  || exit 1;\n",
    "cp data/local/dict/nonsilence_phones.txt $dir/nonsilence_phones.txt\n",
    "\n",
    "#( echo sil; echo spn; echo nsn; echo lau ) > $dir/silence_phones.txt\n",
    "#( echo SIL ; echo SPN ; ) > $dir/silence_phones.txt\n",
    "cp data/local/dict/silence_phones.txt $dir/silence_phones.txt\n",
    "\n",
    "#echo SIL > $dir/optional_silence.txt\n",
    "cp data/local/dict/optional_silence.txt $dir/optional_silence.txt\n",
    "\n",
    "# No \"extra questions\" in the input to this setup, as we don't\n",
    "# have stress or tone.\n",
    "#echo -n >$dir/extra_questions.txt\n",
    "cp data/local/dict/extra_questions.txt $dir/extra_questions.txt\n",
    "\n",
    "# Add to the lexicon the silences, noises etc.\n",
    "( echo '<sp> SIL' ; echo '<unk> SPN'; ) | cat - $dir/lexicon1.txt  > $dir/lexicon2.txt || exit 1;\n",
    "\n",
    "cp $dir/lexicon2.txt $dir/lexicon.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    "set -e # exit on error\n",
    "\n",
    "rm -fr data/exp/lang_bigtext\n",
    "utils/prepare_lang.sh --num-sil-states 4 data/exp/dict_bigtext \"<unk>\" data/exp/lang_bigtext/tmp data/exp/lang_bigtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    "set -e # exit on error\n",
    "\n",
    "dir=data/exp/lm\n",
    "text=data/TEXT/EVERYTHING_UNIQ\n",
    "lexicon=data/exp/dict_bigtext/lexicon.txt\n",
    "mkdir -p $dir\n",
    "export LC_ALL=C \n",
    "\n",
    "cat $text | gzip -c > $dir/train.all.gz\n",
    "#cut -d' ' -f2- $text | tail -n +$heldout_sent | gzip -c > $dir/train.gz\n",
    "#cut -d' ' -f2- $text | head -n $heldout_sent > $dir/heldout\n",
    "\n",
    "cut -d' ' -f1 $lexicon > $dir/wordlist\n",
    "\n",
    "ngram-count -text $dir/train.all.gz -order 4 -limit-vocab -vocab $dir/wordlist \\\n",
    "  -unk -map-unk \"<unk>\" -kndiscount -interpolate -lm $dir/fph.o4g.kn.gz\n",
    "#echo \"PPL for CSJ LM:\"\n",
    "#ngram -unk -lm $dir/csj.o3g.kn.gz -ppl $dir/heldout\n",
    "#ngram -unk -lm $dir/csj.o3g.kn.gz -ppl $dir/heldout -debug 2 >& $dir/ppl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    "set -e # exit on error\n",
    "\n",
    "# We don't really need all these options for SRILM, since the LM training script\n",
    "# does some of the same processing (e.g. -subset -tolower)\n",
    "srilm_opts=\"-subset -prune-lowprobs -unk -tolower -order 4\"\n",
    "LM=data/exp/lm/fph.o4g.kn.gz\n",
    "rm -fr data/exp/lang_nosilp_fph_4g\n",
    "utils/format_lm_sri.sh --srilm-opts \"$srilm_opts\" \\\n",
    "  data/exp/lang_bigtext $LM data/exp/dict_bigtext/lexicon.txt data/exp/lang_nosilp_fph_4g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash \n",
      "0.291139 -0.660707\n",
      "HCLGa is not stochastic\n",
      "0.135011 -0.387397\n",
      "[info]: final HCLG is not stochastic.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tree-info exp/chain/tdnn7n_robust/tree \n",
      "tree-info exp/chain/tdnn7n_robust/tree \n",
      "make-h-transducer --disambig-syms-out=exp/chain/tdnn7n_robust/graph_fph_4g/disambig_tid.int --transition-scale=1.0 data/exp/lang_nosilp_fph_4g/tmp/ilabels_2_1 exp/chain/tdnn7n_robust/tree exp/chain/tdnn7n_robust/final.mdl \n",
      "fstrmepslocal \n",
      "fsttablecompose exp/chain/tdnn7n_robust/graph_fph_4g/Ha.fst data/exp/lang_nosilp_fph_4g/tmp/CLG_2_1.fst \n",
      "fstminimizeencoded \n",
      "fstrmsymbols exp/chain/tdnn7n_robust/graph_fph_4g/disambig_tid.int \n",
      "fstdeterminizestar --use-log=true \n",
      "fstisstochastic exp/chain/tdnn7n_robust/graph_fph_4g/HCLGa.fst \n",
      "add-self-loops --self-loop-scale=1.0 --reorder=true exp/chain/tdnn7n_robust/final.mdl exp/chain/tdnn7n_robust/graph_fph_4g/HCLGa.fst \n",
      "fstisstochastic exp/chain/tdnn7n_robust/graph_fph_4g/HCLG.fst \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 ms, sys: 4 ms, total: 28 ms\n",
      "Wall time: 5min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "#set -euo pipefail\n",
    "\n",
    "# First the options that are passed through to run_ivector_common.sh\n",
    "# (some of which are also used in this script directly).\n",
    "stage=0\n",
    "decode_nj=10\n",
    "train_set=train_robust\n",
    "dev_set=\n",
    "test_sets=\"eval1 eval2 eval3\"\n",
    "gmm=tri4\n",
    "nnet3_affix=\n",
    "\n",
    "# The rest are configs specific to this script. Most of the parameters\n",
    "# are just hardcoded at this level, in the commands below.\n",
    "affix=7n_robust  # affix for the TDNN directory name\n",
    "tree_affix=\n",
    "train_stage=-10\n",
    "get_egs_stage=-10\n",
    "decode_iter=\n",
    "\n",
    "# training options\n",
    "# training chunk-options\n",
    "decode_iter=\n",
    "num_epochs=10\n",
    "initial_effective_lrate=0.001\n",
    "final_effective_lrate=0.0001\n",
    "leftmost_questions_truncate=-1\n",
    "max_param_change=2.0\n",
    "final_layer_normalize_target=0.5\n",
    "num_jobs_initial=1\n",
    "num_jobs_final=3\n",
    "minibatch_size=128,64\n",
    "#frames_per_eg=150,140,100\n",
    "frames_per_eg=75,70,50\n",
    "remove_egs=false\n",
    "common_egs_dir=\n",
    "xent_regularize=0.1\n",
    "\n",
    "test_online_decoding=false  # if true, it will run the last decoding stage.\n",
    "\n",
    "# End configuration section.\n",
    "echo \"$0 $@\"  # Print the command line for logging\n",
    "\n",
    ". ./cmd.sh\n",
    ". ./path.sh\n",
    ". ./utils/parse_options.sh\n",
    "\n",
    "gmm_dir=exp/$gmm\n",
    "ali_dir=exp/${gmm}_ali_${train_set}\n",
    "tree_dir=exp/chain${nnet3_affix}/tree${tree_affix:+_$tree_affix}\n",
    "lang=data/lang_chain\n",
    "lat_dir=exp/chain${nnet3_affix}/${gmm}_${train_set}_lats\n",
    "dir=exp/chain${nnet3_affix}/tdnn${affix}\n",
    "train_data_dir=data/${train_set}_hires\n",
    "lores_train_data_dir=data/${train_set}\n",
    "train_ivector_dir=exp/nnet3${nnet3_affix}/ivectors_${train_set}_hires\n",
    "\n",
    "if [ $stage -le 14 ]; then\n",
    "  # Note: it might appear that this $lang directory is mismatched, and it is as\n",
    "  # far as the 'topo' is concerned, but this script doesn't read the 'topo' from\n",
    "  # the lang directory.\n",
    "  utils/mkgraph.sh --self-loop-scale 1.0 data/exp/lang_nosilp_fph_4g $dir $dir/graph_fph_4g\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
